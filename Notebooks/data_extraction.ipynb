{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "837b26dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://aws:****@rts-358803043452.d.codeartifact.eu-west-1.amazonaws.com/pypi/rts-lib/simple/\n",
      "Requirement already satisfied: boto3 in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (1.40.11)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.11 in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (from boto3) (1.40.11)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (from boto3) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.11->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.11->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.11->boto3) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d082a782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events received: 33\n",
      "Decoded JSON objects: 3832\n",
      "First row example:\n",
      " {\n",
      "  \"properties\": {\n",
      "    \"index\": \"88ad361801fffff\",\n",
      "    \"centroid_lat\": -33.859427322761434,\n",
      "    \"centroid_lon\": 18.677843311941835,\n",
      "    \"resolution\": 8\n",
      "  },\n",
      "  \"geometry\": {\n",
      "    \"type\": \"Polygon\",\n",
      "    \"coordinates\": [\n",
      "      [\n",
      "        [\n",
      "          18.6811898997334,\n",
      "          -33.86330279081797\n",
      "        ],\n",
      "        [\n",
      "          18.683574296194426,\n",
      "          -33.85928287732969\n",
      "        ],\n",
      "        [\n",
      "          18.68022760998973,\n",
      "          -33.85540739558428\n",
      "        ],\n",
      "        [\n",
      "          18.67449676770625,\n",
      "          -33.855551865779326\n",
      "        ],\n",
      "        [\n",
      "          18.672112346191998,\n",
      "          -33.85957172360946\n",
      "        ],\n",
      "        [\n",
      "          18.675458791982372,\n",
      "          -33.8634471669068\n",
      "        ],\n",
      "        [\n",
      "          18.6811898997334,\n",
      "          -33.86330279081797\n",
      "        ]\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "REGION = \"af-south-1\"\n",
    "BUCKET = \"cct-ds-code-challenge-input-data\"\n",
    "KEY    = \"city-hex-polygons-8-10.geojson\"\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT s.properties, s.geometry\n",
    "FROM S3Object[*].features[*] s\n",
    "WHERE s.properties.resolution = 8\n",
    "\"\"\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=REGION, config=Config(s3={\"addressing_style\": \"virtual\"}))\n",
    "\n",
    "resp = s3.select_object_content(\n",
    "    Bucket=BUCKET,\n",
    "    Key=KEY,\n",
    "    ExpressionType=\"SQL\",\n",
    "    Expression=sql,\n",
    "    InputSerialization={\"JSON\": {\"Type\": \"DOCUMENT\"}},\n",
    "    OutputSerialization={\"JSON\": {\"RecordDelimiter\": \"\\n\"}},  # may still split objects across chunks\n",
    ")\n",
    "\n",
    "decoder = json.JSONDecoder()\n",
    "buffer = \"\"     # accumulated text across streaming chunks\n",
    "records = []\n",
    "n_events = 0\n",
    "n_decoded = 0\n",
    "\n",
    "def drain_buffer(buf: str):\n",
    "    \"\"\"Yield as many JSON objects as possible from the head of buf, return (objs, remainder).\"\"\"\n",
    "    objs = []\n",
    "    i = 0\n",
    "    L = len(buf)\n",
    "    while True:\n",
    "        # Skip whitespace/newlines between objects\n",
    "        while i < L and buf[i].isspace():\n",
    "            i += 1\n",
    "        if i >= L:\n",
    "            break\n",
    "        try:\n",
    "            obj, end = decoder.raw_decode(buf, i)\n",
    "            objs.append(obj)\n",
    "            i = end\n",
    "        except json.JSONDecodeError:\n",
    "            # Need more data to complete the next object\n",
    "            break\n",
    "    return objs, buf[i:]\n",
    "\n",
    "for event in resp[\"Payload\"]:\n",
    "    n_events += 1\n",
    "    if \"Records\" in event:\n",
    "        chunk = event[\"Records\"][\"Payload\"].decode(\"utf-8\", errors=\"replace\")\n",
    "        buffer += chunk\n",
    "\n",
    "        # Try to decode as many complete objects as possible from the buffer\n",
    "        objs, buffer = drain_buffer(buffer)\n",
    "        if objs:\n",
    "            records.extend(objs)\n",
    "            n_decoded += len(objs)\n",
    "\n",
    "    elif \"Stats\" in event or \"Progress\" in event:\n",
    "        # Optional: you can inspect event[\"Stats\"] / [\"Progress\"] for bytes processed\n",
    "        pass\n",
    "    elif \"End\" in event:\n",
    "        break\n",
    "\n",
    "# After stream ends, there might still be leftover text in the buffer (e.g., trailing whitespace)\n",
    "leftover = buffer.strip()\n",
    "\n",
    "if leftover:\n",
    "    # Try one last time, then print debug if it still doesn't parse\n",
    "    try:\n",
    "        objs, buffer = drain_buffer(leftover)\n",
    "        records.extend(objs)\n",
    "        n_decoded += len(objs)\n",
    "        leftover = buffer.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Events received: {n_events}\")\n",
    "print(f\"Decoded JSON objects: {n_decoded}\")\n",
    "\n",
    "if leftover:\n",
    "    print(\"\\n--- Leftover (did not parse to complete JSON object) ---\")\n",
    "    print(leftover[:1000])  # show first 1000 chars for debugging\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "if records:\n",
    "    print(\"First row example:\\n\", json.dumps(records[0], indent=2)[:1000])\n",
    "else:\n",
    "    print(\"No records decoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ecb6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://aws:****@rts-358803043452.d.codeartifact.eu-west-1.amazonaws.com/pypi/rts-lib/simple/\n",
      "Requirement already satisfied: orjson in /Users/ben/Documents/test/.venv/lib/python3.10/site-packages (3.11.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cfb0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 22:07:49 | INFO     | Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-19 22:07:52 | INFO     | Rows written: 3832 | Events: 35 | Total wall time: 2.568 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "REGION = \"af-south-1\"\n",
    "BUCKET = \"cct-ds-code-challenge-input-data\"\n",
    "KEY    = \"city-hex-polygons-8-10.geojson\"\n",
    "\n",
    "OUT_JSONL = \"hex8_features.jsonl\"   # one Feature per line (ALL fields preserved)\n",
    "\n",
    "DEBUG_MODE = False                  # flip True for full timings & resource logs\n",
    "LOG_EVERY_N_EVENTS = 25             # event logging interval (only in debug mode)\n",
    "SAMPLE_VALIDATE_EVERY = 1000        # ~1 sampled validation per N lines in debug mode\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG if DEBUG_MODE else logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"hex8_allfields_json_fast\")\n",
    "\n",
    "SQL = \"\"\"\n",
    "SELECT s\n",
    "FROM S3Object[*].features[*] s\n",
    "WHERE s.properties.resolution = 8\n",
    "\"\"\"\n",
    "\n",
    "@contextmanager\n",
    "def timed(label: str):\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if DEBUG_MODE:\n",
    "            log.debug(\"%s took %.3f s\", label, time.perf_counter() - t0)\n",
    "\n",
    "def main():\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    with timed(\"S3 client init\"):\n",
    "        s3 = boto3.client(\"s3\", region_name=REGION, config=Config(s3={\"addressing_style\": \"virtual\"}))\n",
    "\n",
    "    with timed(\"S3 Select open\"):\n",
    "        resp = s3.select_object_content(\n",
    "            Bucket=BUCKET,\n",
    "            Key=KEY,\n",
    "            ExpressionType=\"SQL\",\n",
    "            Expression=SQL,\n",
    "            InputSerialization={\"JSON\": {\"Type\": \"DOCUMENT\"}},\n",
    "            OutputSerialization={\"JSON\": {\"RecordDelimiter\": \"\\n\"}},\n",
    "        )\n",
    "\n",
    "    events = 0\n",
    "    rows = 0\n",
    "    bad_samples = 0\n",
    "    bytes_scanned = bytes_processed = bytes_returned = None\n",
    "\n",
    "    buf = bytearray()\n",
    "\n",
    "    with timed(\"Stream + parse + write\"), open(OUT_JSONL, \"wb\", buffering=8 * 1024 * 1024) as fout:\n",
    "        for event in resp[\"Payload\"]:\n",
    "            events += 1\n",
    "\n",
    "            rec = event.get(\"Records\")\n",
    "            if rec:\n",
    "                payload = rec[\"Payload\"]  # bytes\n",
    "                buf.extend(payload)\n",
    "\n",
    "                last_nl = buf.rfind(b\"\\n\")\n",
    "                if last_nl != -1:\n",
    "                    to_write = buf[: last_nl + 1]  # bytes slice\n",
    "                    if DEBUG_MODE and SAMPLE_VALIDATE_EVERY > 0 and rows % SAMPLE_VALIDATE_EVERY == 0:\n",
    "                        prev_nl = to_write.rfind(b\"\\n\", 0, len(to_write) - 1)\n",
    "                        sample = to_write[prev_nl + 1 : -1] if prev_nl != -1 else to_write[:-1]\n",
    "                        try:\n",
    "                            json.loads(sample.decode(\"utf-8\"))\n",
    "                        except Exception as e:\n",
    "                            bad_samples += 1\n",
    "                            log.debug(\"Sample validation failed: %s\", e)\n",
    "\n",
    "                    fout.write(to_write)\n",
    "                    rows += to_write.count(b\"\\n\")\n",
    "                    del buf[: last_nl + 1]\n",
    "\n",
    "            elif \"Stats\" in event:\n",
    "                d = event[\"Stats\"][\"Details\"]\n",
    "                bytes_scanned   = d.get(\"BytesScanned\")\n",
    "                bytes_processed = d.get(\"BytesProcessed\")\n",
    "                bytes_returned  = d.get(\"BytesReturned\")\n",
    "\n",
    "            if DEBUG_MODE and events % LOG_EVERY_N_EVENTS == 0:\n",
    "                log.debug(\"Event %d | total rows=%d | buffer=%d bytes\",\n",
    "                          events, rows, len(buf))\n",
    "\n",
    "        if buf:\n",
    "            if buf[-1:] != b\"\\n\":\n",
    "                buf += b\"\\n\"\n",
    "            if DEBUG_MODE and SAMPLE_VALIDATE_EVERY > 0:\n",
    "                try:\n",
    "                    last_nl = buf[:-1].rfind(b\"\\n\")\n",
    "                    sample = buf[last_nl + 1 : -1] if last_nl != -1 else buf[:-1]\n",
    "                    json.loads(sample.decode(\"utf-8\"))\n",
    "                except Exception as e:\n",
    "                    bad_samples += 1\n",
    "                    log.debug(\"Final sample validation failed: %s\", e)\n",
    "            fout.write(buf)\n",
    "            rows += buf.count(b\"\\n\")\n",
    "\n",
    "    wall_total = time.perf_counter() - total_start\n",
    "\n",
    "    # Always-on summary line\n",
    "    log.info(\"Rows written: %d | Events: %d | Total wall time: %.3f s\", rows, events, wall_total)\n",
    "\n",
    "    if DEBUG_MODE:\n",
    "        log.debug(\"======== SUMMARY (json-fast) ========\")\n",
    "        log.debug(\"Output file: %s\", OUT_JSONL)\n",
    "        log.debug(\"S3 Select bytes (MB): scanned=%.2f processed=%.2f returned=%.2f\",\n",
    "                  (bytes_scanned or 0) / (1024**2),\n",
    "                  (bytes_processed or 0) / (1024**2),\n",
    "                  (bytes_returned or 0) / (1024**2))\n",
    "        if bad_samples:\n",
    "            log.debug(\"Sampled validation failures: %d\", bad_samples)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc40ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 22:07:29 | INFO     | Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-08-19 22:07:29 | INFO     | S3 client init took 0.115 s\n",
      "2025-08-19 22:07:29 | INFO     | S3 Select open took 0.119 s\n",
      "2025-08-19 22:07:32 | INFO     | Stream + parse + write took 2.211 s\n",
      "2025-08-19 22:07:32 | INFO     | ======== SUMMARY ========\n",
      "2025-08-19 22:07:32 | INFO     | Output file: hex8_features.jsonl\n",
      "2025-08-19 22:07:32 | INFO     | Rows written: 3832 | Events: 35\n",
      "2025-08-19 22:07:32 | INFO     | Total wall time: 2.448 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "REGION = \"af-south-1\"\n",
    "BUCKET = \"cct-ds-code-challenge-input-data\"\n",
    "KEY    = \"city-hex-polygons-8-10.geojson\"\n",
    "\n",
    "OUT_JSONL = \"hex8_features.jsonl\"   # one Feature per line (ALL fields preserved)\n",
    "\n",
    "DEBUG_MODE = False                  # detailed resource logs only if True\n",
    "LOG_EVERY_N_EVENTS = 25             # higher = less logger overhead\n",
    "SAMPLE_VALIDATE_EVERY = 1000        # ~1 sampled validation per N lines in DEBUG_MODE\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG if DEBUG_MODE else logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"hex8_allfields_json_fast\")\n",
    "\n",
    "SQL = \"\"\"\n",
    "SELECT s\n",
    "FROM S3Object[*].features[*] s\n",
    "WHERE s.properties.resolution = 8\n",
    "\"\"\"\n",
    "\n",
    "@contextmanager\n",
    "def timed(label: str):\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        log.info(\"%s took %.3f s\", label, time.perf_counter() - t0)\n",
    "\n",
    "def main():\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    with timed(\"S3 client init\"):\n",
    "        s3 = boto3.client(\"s3\", region_name=REGION, config=Config(s3={\"addressing_style\": \"virtual\"}))\n",
    "\n",
    "    with timed(\"S3 Select open\"):\n",
    "        resp = s3.select_object_content(\n",
    "            Bucket=BUCKET,\n",
    "            Key=KEY,\n",
    "            ExpressionType=\"SQL\",\n",
    "            Expression=SQL,  # boto3 expects str\n",
    "            InputSerialization={\"JSON\": {\"Type\": \"DOCUMENT\"}},\n",
    "            OutputSerialization={\"JSON\": {\"RecordDelimiter\": \"\\n\"}},\n",
    "        )\n",
    "\n",
    "    events = 0\n",
    "    rows = 0\n",
    "    bad_samples = 0\n",
    "    bytes_scanned = bytes_processed = bytes_returned = None\n",
    "\n",
    "    # Keep a bytearray buffer and write complete lines in large chunks\n",
    "    buf = bytearray()\n",
    "\n",
    "    # Large OS buffer for fewer syscalls\n",
    "    with timed(\"Stream + parse + write\"), open(OUT_JSONL, \"wb\", buffering=8 * 1024 * 1024) as fout:\n",
    "        for event in resp[\"Payload\"]:\n",
    "            events += 1\n",
    "            ev_t0 = time.perf_counter()\n",
    "\n",
    "            rec = event.get(\"Records\")\n",
    "            if rec:\n",
    "                payload = rec[\"Payload\"]  # bytes\n",
    "                buf.extend(payload)\n",
    "\n",
    "                # Find last newline and flush everything up to it\n",
    "                last_nl = buf.rfind(b\"\\n\")\n",
    "                if last_nl != -1:\n",
    "                    to_write = buf[: last_nl + 1]  # bytes slice (has .count)\n",
    "                    if DEBUG_MODE and SAMPLE_VALIDATE_EVERY > 0 and rows % SAMPLE_VALIDATE_EVERY == 0:\n",
    "                        # Sample-validate one full line from this chunk (avoid splitting)\n",
    "                        prev_nl = to_write.rfind(b\"\\n\", 0, len(to_write) - 1)\n",
    "                        sample = to_write[prev_nl + 1 : -1] if prev_nl != -1 else to_write[:-1]\n",
    "                        try:\n",
    "                            json.loads(sample.decode(\"utf-8\"))\n",
    "                        except Exception as e:\n",
    "                            bad_samples += 1\n",
    "                            log.debug(\"Sample validation failed: %s\", e)\n",
    "\n",
    "                    fout.write(to_write)                # one big write\n",
    "                    rows += to_write.count(b\"\\n\")       # cheap row count\n",
    "                    del buf[: last_nl + 1]              # drop written bytes in-place\n",
    "\n",
    "            elif \"Stats\" in event:\n",
    "                d = event[\"Stats\"][\"Details\"]\n",
    "                bytes_scanned   = d.get(\"BytesScanned\")\n",
    "                bytes_processed = d.get(\"BytesProcessed\")\n",
    "                bytes_returned  = d.get(\"BytesReturned\")\n",
    "\n",
    "            if DEBUG_MODE and events % LOG_EVERY_N_EVENTS == 0:\n",
    "                log.debug(\"Event %d processed in %.4f s | total rows=%d | buffer=%d bytes\",\n",
    "                          events, time.perf_counter() - ev_t0, rows, len(buf))\n",
    "\n",
    "        # Write any trailing line (if stream didn't end with '\\n')\n",
    "        if buf:\n",
    "            if buf[-1:] != b\"\\n\":\n",
    "                buf += b\"\\n\"\n",
    "            if DEBUG_MODE and SAMPLE_VALIDATE_EVERY > 0:\n",
    "                try:\n",
    "                    last_nl = buf[:-1].rfind(b\"\\n\")\n",
    "                    sample = buf[last_nl + 1 : -1] if last_nl != -1 else buf[:-1]\n",
    "                    json.loads(sample.decode(\"utf-8\"))\n",
    "                except Exception as e:\n",
    "                    bad_samples += 1\n",
    "                    log.debug(\"Final sample validation failed: %s\", e)\n",
    "            fout.write(buf)\n",
    "            rows += buf.count(b\"\\n\")\n",
    "\n",
    "    wall_total = time.perf_counter() - total_start\n",
    "\n",
    "    # Minimal always-on summary\n",
    "    log.info(\"======== SUMMARY ========\")\n",
    "    log.info(\"Output file: %s\", OUT_JSONL)\n",
    "    log.info(\"Rows written: %d | Events: %d\", rows, events)\n",
    "    log.info(\"Total wall time: %.3f s\", wall_total)\n",
    "\n",
    "    if DEBUG_MODE:\n",
    "        log.debug(\"S3 Select bytes (MB): scanned=%.2f processed=%.2f returned=%.2f\",\n",
    "                  (bytes_scanned or 0) / (1024**2),\n",
    "                  (bytes_processed or 0) / (1024**2),\n",
    "                  (bytes_returned or 0) / (1024**2))\n",
    "        if bad_samples:\n",
    "            log.debug(\"Sampled validation failures: %d\", bad_samples)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22417749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 22:09:16 | INFO     | Events: 35 | Total wall time: 2.513 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "REGION = \"af-south-1\"\n",
    "BUCKET = \"cct-ds-code-challenge-input-data\"\n",
    "KEY    = \"city-hex-polygons-8-10.geojson\"\n",
    "\n",
    "OUT_JSONL = \"hex8_features.jsonl\"   # one Feature per line (ALL fields preserved)\n",
    "\n",
    "DEBUG_MODE = False                  # flip True for detailed logs, row counts, sampled validation\n",
    "LOG_EVERY_N_EVENTS = 25\n",
    "SAMPLE_VALIDATE_EVERY = 1000        # ~1 sampled validation per N lines in debug mode\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG if DEBUG_MODE else logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "log = logging.getLogger(\"hex8_allfields_json_tail\")\n",
    "\n",
    "SQL = \"\"\"\n",
    "SELECT s\n",
    "FROM S3Object[*].features[*] s\n",
    "WHERE s.properties.resolution = 8\n",
    "\"\"\"\n",
    "\n",
    "@contextmanager\n",
    "def timed(label: str):\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if DEBUG_MODE:\n",
    "            log.debug(\"%s took %.3f s\", label, time.perf_counter() - t0)\n",
    "\n",
    "def main():\n",
    "    total_start = time.perf_counter()\n",
    "\n",
    "    with timed(\"S3 client init\"):\n",
    "        s3 = boto3.client(\"s3\", region_name=REGION, config=Config(s3={\"addressing_style\": \"virtual\"}))\n",
    "\n",
    "    with timed(\"S3 Select open\"):\n",
    "        resp = s3.select_object_content(\n",
    "            Bucket=BUCKET,\n",
    "            Key=KEY,\n",
    "            ExpressionType=\"SQL\",\n",
    "            Expression=SQL,\n",
    "            InputSerialization={\"JSON\": {\"Type\": \"DOCUMENT\"}},\n",
    "            OutputSerialization={\"JSON\": {\"RecordDelimiter\": \"\\n\"}},\n",
    "        )\n",
    "\n",
    "    events = 0\n",
    "    rows = 0        # only incremented in DEBUG_MODE\n",
    "    bad_samples = 0\n",
    "\n",
    "    # Keep only the partial line between chunks\n",
    "    tail = b\"\"\n",
    "\n",
    "    with timed(\"Stream + parse + write\"), open(OUT_JSONL, \"wb\", buffering=8 * 1024 * 1024) as fout:\n",
    "        for event in resp[\"Payload\"]:\n",
    "            events += 1\n",
    "\n",
    "            rec = event.get(\"Records\")\n",
    "            if rec:\n",
    "                payload = rec[\"Payload\"]  # bytes\n",
    "\n",
    "                # Concatenate previous tail with the new payload (1 new bytes object)\n",
    "                data = tail + payload\n",
    "\n",
    "                # Find last complete newline\n",
    "                last_nl = data.rfind(b\"\\n\")\n",
    "                if last_nl != -1:\n",
    "                    # Zero-copy write of all complete lines\n",
    "                    chunk_view = memoryview(data)[: last_nl + 1]\n",
    "                    if DEBUG_MODE and SAMPLE_VALIDATE_EVERY > 0 and rows % SAMPLE_VALIDATE_EVERY == 0:\n",
    "                        # Sample-validate the last full line in this block\n",
    "                        prev_nl = data.rfind(b\"\\n\", 0, last_nl)\n",
    "                        sample = data[prev_nl + 1 : last_nl] if prev_nl != -1 else data[:last_nl]\n",
    "                        try:\n",
    "                            json.loads(sample.decode(\"utf-8\"))\n",
    "                        except Exception as e:\n",
    "                            bad_samples += 1\n",
    "                            log.debug(\"Sample validation failed: %s\", e)\n",
    "\n",
    "                    fout.write(chunk_view)\n",
    "\n",
    "                    if DEBUG_MODE:\n",
    "                        rows += data.count(b\"\\n\", 0, last_nl + 1)  # count only in debug\n",
    "\n",
    "                    # Keep the remainder (after the last newline) as the new tail\n",
    "                    tail = data[last_nl + 1 :]\n",
    "                else:\n",
    "                    # No newline in this payload; carry everything as tail\n",
    "                    tail = data\n",
    "\n",
    "            elif \"Stats\" in event and DEBUG_MODE:\n",
    "                d = event[\"Stats\"][\"Details\"]\n",
    "                log.debug(\"Stats: scanned=%.2fMB processed=%.2fMB returned=%.2fMB\",\n",
    "                          d.get(\"BytesScanned\", 0)/1048576,\n",
    "                          d.get(\"BytesProcessed\", 0)/1048576,\n",
    "                          d.get(\"BytesReturned\", 0)/1048576)\n",
    "\n",
    "            if DEBUG_MODE and events % LOG_EVERY_N_EVENTS == 0:\n",
    "                log.debug(\"Event %d | rows=%d | tail=%d bytes\", events, rows, len(tail))\n",
    "\n",
    "        # Flush any trailing partial line (ensure trailing newline)\n",
    "        if tail:\n",
    "            if tail[-1:] != b\"\\n\":\n",
    "                fout.write(tail + b\"\\n\")\n",
    "                if DEBUG_MODE:\n",
    "                    rows += 1\n",
    "            else:\n",
    "                fout.write(tail)\n",
    "                if DEBUG_MODE:\n",
    "                    rows += tail.count(b\"\\n\")\n",
    "\n",
    "    wall_total = time.perf_counter() - total_start\n",
    "\n",
    "    # Always-on minimal summary\n",
    "    if DEBUG_MODE:\n",
    "        log.info(\"Rows written: %d | Events: %d | Total wall time: %.3f s\",\n",
    "                rows, events, wall_total)\n",
    "    else:\n",
    "        log.info(\"Events: %d | Total wall time: %.3f s\",\n",
    "                events, wall_total)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944143e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3832, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>resolution</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88ad361801fffff</td>\n",
       "      <td>-33.859427</td>\n",
       "      <td>18.677843</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((18.68119 -33.8633, 18.68357 -33.8592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88ad361803fffff</td>\n",
       "      <td>-33.855696</td>\n",
       "      <td>18.668766</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((18.67211 -33.85957, 18.6745 -33.8555...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88ad361805fffff</td>\n",
       "      <td>-33.855263</td>\n",
       "      <td>18.685959</td>\n",
       "      <td>8</td>\n",
       "      <td>POLYGON ((18.68931 -33.85914, 18.69169 -33.855...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  centroid_lat  centroid_lon  resolution  \\\n",
       "0  88ad361801fffff    -33.859427     18.677843           8   \n",
       "1  88ad361803fffff    -33.855696     18.668766           8   \n",
       "2  88ad361805fffff    -33.855263     18.685959           8   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((18.68119 -33.8633, 18.68357 -33.8592...  \n",
       "1  POLYGON ((18.67211 -33.85957, 18.6745 -33.8555...  \n",
       "2  POLYGON ((18.68931 -33.85914, 18.69169 -33.855...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL 2 — Robust loader: JSONL -> GeoPandas (handles wrapped/columnar shapes)\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "def gpd_from_jsonl(path: str, debug: bool = False) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Accepts JSON Lines where each line is either:\n",
    "      • a GeoJSON Feature: {\"type\":\"Feature\",\"properties\":{...},\"geometry\":{...}}\n",
    "      • wrapped: {\"s\": {Feature...}}\n",
    "      • columnar: {\"_1\": properties, \"_2\": geometry}\n",
    "      • or another single-key wrapper around a Feature\n",
    "\n",
    "    Returns a GeoDataFrame with geometry and flattened properties.\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    wrapped = colstyle = skipped = 0\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln, line in enumerate(f, 1):\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            obj = json.loads(s)\n",
    "\n",
    "            rec = None\n",
    "            if isinstance(obj, dict) and \"geometry\" in obj and \"properties\" in obj:\n",
    "                rec = obj\n",
    "            elif isinstance(obj, dict) and \"s\" in obj and isinstance(obj[\"s\"], dict):\n",
    "                rec = obj[\"s\"]; wrapped += 1\n",
    "            elif isinstance(obj, dict) and \"_1\" in obj and \"_2\" in obj:\n",
    "                # Map column-style into a proper Feature\n",
    "                rec = {\"type\": \"Feature\", \"properties\": obj[\"_1\"], \"geometry\": obj[\"_2\"]}; colstyle += 1\n",
    "            else:\n",
    "                # Try any single nested dict that looks like a Feature\n",
    "                if isinstance(obj, dict):\n",
    "                    for v in obj.values():\n",
    "                        if isinstance(v, dict) and \"geometry\" in v and \"properties\" in v:\n",
    "                            rec = v; wrapped += 1\n",
    "                            break\n",
    "\n",
    "            if not isinstance(rec, dict) or \"geometry\" not in rec:\n",
    "                skipped += 1\n",
    "                if debug and skipped <= 3:\n",
    "                    print(f\"Skipping line {ln}: no 'geometry' key\")\n",
    "                continue\n",
    "\n",
    "            feats.append(rec)\n",
    "\n",
    "    if not feats:\n",
    "        raise ValueError(\"No usable Feature records with 'geometry' found. Inspect your JSONL lines.\")\n",
    "\n",
    "    # Build columns\n",
    "    props = [feat.get(\"properties\", {}) for feat in feats]\n",
    "    geoms = [shape(feat[\"geometry\"]) if feat.get(\"geometry\") is not None else None for feat in feats]\n",
    "\n",
    "    df_props = pd.json_normalize(props, sep=\".\")\n",
    "    gdf = gpd.GeoDataFrame(df_props, geometry=geoms, crs=\"EPSG:4326\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Loaded {len(feats)} features | wrapped={wrapped} | columnar={colstyle} | skipped={skipped}\")\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# --- Use it ---\n",
    "IN_JSONL = \"hex8_features.jsonl\"\n",
    "gdf = gpd_from_jsonl(IN_JSONL, debug=False)  # flip to True for a brief diagnostics print\n",
    "print(gdf.shape)\n",
    "gdf.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aeeb8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[0] if \"__file__\" in globals() else Path().resolve()\n",
    "while ROOT.name != \"ds_code_challenge\" and ROOT.parent != ROOT:\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "file_map = {\n",
    "    \"sr.csv\": \"df_sr\",\n",
    "    \"sr_hex.csv\": \"df_sr_hex\",\n",
    "    \"sr_hex_truncated.csv\": \"df_sr_hex_truncated\",\n",
    "    \"city-hex-polygons-8.geojson\": \"gdf_city_hex_8\"\n",
    "}\n",
    "\n",
    "# Load the files\n",
    "for file_name, var_name in file_map.items():\n",
    "    file_path = DATA_DIR / file_name\n",
    "\n",
    "\n",
    "    if file_path.suffix == \".csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "        globals()[var_name] = df\n",
    "\n",
    "    elif file_path.suffix == \".geojson\":\n",
    "        gdf = gpd.read_file(file_path)\n",
    "        globals()[var_name] = gdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1847d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from typing import Dict\n",
    "\n",
    "def _ensure_key_col(gdf: gpd.GeoDataFrame, key: str = \"index\") -> gpd.GeoDataFrame:\n",
    "    return gdf if key in gdf.columns else gdf.reset_index().rename(columns={\"index\": key})\n",
    "\n",
    "def compare_hex_gdfs_simple(\n",
    "    left: gpd.GeoDataFrame,\n",
    "    right: gpd.GeoDataFrame,\n",
    "    key: str = \"index\",\n",
    "    geom_tolerance: float = 0.0,  # 0 = strict; >0 means distance <= tol (CRS units) counts as equal\n",
    "    na_equal: bool = True,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    # Make sure key column exists\n",
    "    left  = _ensure_key_col(left, key)\n",
    "    right = _ensure_key_col(right, key)\n",
    "\n",
    "    required = {key, \"centroid_lat\", \"centroid_lon\", \"geometry\"}\n",
    "    for name, df in ((\"left\", left), (\"right\", right)):\n",
    "        missing = sorted(required - set(df.columns))\n",
    "        if missing:\n",
    "            raise ValueError(f\"{name} GeoDataFrame missing columns: {missing}\")\n",
    "\n",
    "    # Select/clone\n",
    "    geom_l = left.geometry.name\n",
    "    geom_r = right.geometry.name\n",
    "    L = left[[key, \"centroid_lat\", \"centroid_lon\", geom_l]].copy()\n",
    "    R = right[[key, \"centroid_lat\", \"centroid_lon\", geom_r]].copy()\n",
    "\n",
    "    # Align CRS (reproject right -> left if both set and differ)\n",
    "    if getattr(left, \"crs\", None) and getattr(right, \"crs\", None) and left.crs != right.crs:\n",
    "        R = gpd.GeoDataFrame(R, geometry=geom_r, crs=right.crs).to_crs(left.crs)\n",
    "\n",
    "    # Merge on key\n",
    "    m = L.merge(R, on=key, how=\"outer\", suffixes=(\"_l\", \"_r\"), indicator=True)\n",
    "\n",
    "    only_in_left  = m.loc[m[\"_merge\"] == \"left_only\",  [key]].reset_index(drop=True)\n",
    "    only_in_right = m.loc[m[\"_merge\"] == \"right_only\", [key]].reset_index(drop=True)\n",
    "    both = m.loc[m[\"_merge\"] == \"both\"].copy()\n",
    "\n",
    "    # Element-wise attr equality\n",
    "    def _eq(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "        out = (a == b)\n",
    "        return out | (a.isna() & b.isna()) if na_equal else out\n",
    "\n",
    "    lat_eq = _eq(both[\"centroid_lat_l\"], both[\"centroid_lat_r\"])\n",
    "    lon_eq = _eq(both[\"centroid_lon_l\"], both[\"centroid_lon_r\"])\n",
    "\n",
    "    # Element-wise geometry equality\n",
    "    g1 = gpd.GeoSeries(both[f\"{geom_l}_l\"], crs=left.crs)\n",
    "    g2 = gpd.GeoSeries(both[f\"{geom_r}_r\"], crs=left.crs)  # already reprojected if needed\n",
    "\n",
    "    if geom_tolerance <= 0:\n",
    "        geom_eq = g1.geom_equals(g2)\n",
    "        if na_equal:\n",
    "            geom_eq = geom_eq | (g1.isna() & g2.isna())\n",
    "    else:\n",
    "        def _within(a: BaseGeometry, b: BaseGeometry) -> bool:\n",
    "            if a is None or b is None:\n",
    "                return na_equal and (a is None and b is None)\n",
    "            try:\n",
    "                return a.distance(b) <= geom_tolerance\n",
    "            except Exception:\n",
    "                return False\n",
    "        geom_eq = pd.Series([_within(a, b) for a, b in zip(g1.values, g2.values)], index=both.index)\n",
    "\n",
    "    # Overall equality\n",
    "    all_eq = lat_eq & lon_eq & geom_eq\n",
    "    matches = both.loc[all_eq, [key]].reset_index(drop=True)\n",
    "    diffs   = both.loc[~all_eq].copy()\n",
    "\n",
    "    # Long-form diffs\n",
    "    rows = []\n",
    "    if (~lat_eq).any():\n",
    "        t = diffs.loc[diffs.index.intersection(both.index[~lat_eq]), [key, \"centroid_lat_l\", \"centroid_lat_r\"]].copy()\n",
    "        t.insert(1, \"column\", \"centroid_lat\")\n",
    "        t.rename(columns={\"centroid_lat_l\": \"left\", \"centroid_lat_r\": \"right\"}, inplace=True)\n",
    "        rows.append(t)\n",
    "    if (~lon_eq).any():\n",
    "        t = diffs.loc[diffs.index.intersection(both.index[~lon_eq]), [key, \"centroid_lon_l\", \"centroid_lon_r\"]].copy()\n",
    "        t.insert(1, \"column\", \"centroid_lon\")\n",
    "        t.rename(columns={\"centroid_lon_l\": \"left\", \"centroid_lon_r\": \"right\"}, inplace=True)\n",
    "        rows.append(t)\n",
    "    if (~geom_eq).any():\n",
    "        t = diffs.loc[diffs.index.intersection(both.index[~geom_eq]), [key, f\"{geom_l}_l\", f\"{geom_r}_r\"]].copy()\n",
    "        t.insert(1, \"column\", \"geometry\")\n",
    "        t[\"left\"]  = gpd.GeoSeries(t.pop(f\"{geom_l}_l\"), crs=left.crs).to_wkt()\n",
    "        t[\"right\"] = gpd.GeoSeries(t.pop(f\"{geom_r}_r\"), crs=left.crs).to_wkt()\n",
    "        rows.append(t[[key, \"column\", \"left\", \"right\"]])\n",
    "\n",
    "    mismatches_long = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[key, \"column\", \"left\", \"right\"])\n",
    "\n",
    "    # Wide-form for convenience\n",
    "    if not diffs.empty:\n",
    "        keep = [key, \"centroid_lat_l\", \"centroid_lat_r\", \"centroid_lon_l\", \"centroid_lon_r\", f\"{geom_l}_l\", f\"{geom_r}_r\"]\n",
    "        mismatches_wide = diffs[[c for c in keep if c in diffs.columns]].reset_index(drop=True)\n",
    "    else:\n",
    "        mismatches_wide = pd.DataFrame(columns=[key])\n",
    "\n",
    "    return {\n",
    "        \"only_in_left\": only_in_left,\n",
    "        \"only_in_right\": only_in_right,\n",
    "        \"matches\": matches,\n",
    "        \"mismatches_long\": mismatches_long,\n",
    "        \"mismatches_wide\": mismatches_wide,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32829ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only in left: 0\n",
      "Only in right: 0\n",
      "Matches: 3832\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "res = compare_hex_gdfs_simple(gdf_city_hex_8, gdf, key=\"index\", geom_tolerance=0.0) \n",
    "\n",
    "print(\"Only in left:\", len(res[\"only_in_left\"]))\n",
    "print(\"Only in right:\", len(res[\"only_in_right\"]))\n",
    "print(\"Matches:\", len(res[\"matches\"]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
