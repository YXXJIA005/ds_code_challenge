{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a579bb6c",
   "metadata": {},
   "source": [
    "# Notebook 1\n",
    "This notebook sets up the AWS details, downloads data from s3 using select and download relevant all files provided in the original readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "# --- Stdlib ---\n",
    "import contextlib\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, Optional, Tuple\n",
    "\n",
    "# --- AWS / S3 ---\n",
    "import boto3\n",
    "from botocore.config import Config \n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Data Science ---\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Geo ---\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "\n",
    "# --- Testing ---\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "# --- Optional Diagnostics ---\n",
    "try:\n",
    "    import psutil as _psutil  # type: ignore\n",
    "except Exception:\n",
    "    _psutil = None\n",
    "\n",
    "try:\n",
    "    import tracemalloc as _tracemalloc  # type: ignore\n",
    "except Exception:\n",
    "    _tracemalloc = None\n",
    "\n",
    "# --- Config ---\n",
    "ipytest.autoconfig()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)-8s | %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def ensure_dir(path: str | os.PathLike) -> None:\n",
    "    os.makedirs(path if isinstance(path, str) else str(path), exist_ok=True)\n",
    "\n",
    "def parent_dir_of(path: str | os.PathLike) -> str:\n",
    "    return os.path.dirname(str(path)) or \".\"\n",
    "\n",
    "def fmt_bytes(n: int | float) -> str:\n",
    "    try:\n",
    "        n = float(n)\n",
    "    except Exception:\n",
    "        return str(n)\n",
    "    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"):\n",
    "        if abs(n) < 1024 or unit == \"PB\":\n",
    "            return f\"{n:.1f}{unit}\"\n",
    "        n /= 1024\n",
    "\n",
    "def gunzip_file(src: str, dst: str) -> None:\n",
    "    ensure_dir(parent_dir_of(dst))\n",
    "    with gzip.open(src, \"rb\") as fin, open(dst, \"wb\") as fout:\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "\n",
    "def client_error_msg(e) -> str:\n",
    "    code = getattr(e, \"response\", {}).get(\"Error\", {}).get(\"Code\")\n",
    "    msg  = getattr(e, \"response\", {}).get(\"Error\", {}).get(\"Message\")\n",
    "    return f\"[{code}] {msg}\" if code or msg else str(e)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timed(label: str, enabled: bool = True):\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if enabled:\n",
    "            logger.info(\"%s took %.3f s\", label, time.perf_counter() - t0)\n",
    "\n",
    "def resource_snapshot(note: str = \"\") -> None:\n",
    "    if not logger.isEnabledFor(logging.DEBUG):\n",
    "        return\n",
    "    parts: list[str] = []\n",
    "    if _psutil:\n",
    "        try:\n",
    "            p = _psutil.Process(os.getpid())\n",
    "            parts.append(f\"rss={fmt_bytes(p.memory_info().rss)}\")\n",
    "            parts.append(f\"cpu%~{p.cpu_percent(interval=0.0):.1f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if _tracemalloc and _tracemalloc.is_tracing():\n",
    "        try:\n",
    "            cur, peak = _tracemalloc.get_traced_memory()\n",
    "            parts.append(f\"py_mem={fmt_bytes(cur)}/{fmt_bytes(peak)}(peak)\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if parts:\n",
    "        logger.debug(\"RES%s %s\", f\"[{note}]\" if note else \"\", \" \".join(parts))\n",
    "def download_file(\n",
    "    url: str,\n",
    "    save_as: str,\n",
    "    overwrite: bool = False,\n",
    "    dest_dir: str | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from a URL and saves it locally.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to fetch.\n",
    "        save_as (str): The filename to save as (e.g., 'credentials.json').\n",
    "        overwrite (bool): Whether to overwrite if the file already exists.\n",
    "        dest_dir (str | None): Optional destination directory. Defaults to cwd.\n",
    "\n",
    "    Returns:\n",
    "        str: Absolute path to the saved file.\n",
    "    \"\"\"\n",
    "    dest_path = Path(dest_dir or \".\").resolve() / save_as\n",
    "\n",
    "    if dest_path.exists() and not overwrite:\n",
    "        logger.info(\"download_file: file already exists → %s (skipped)\", dest_path)\n",
    "        return str(dest_path)\n",
    "\n",
    "    with timed(f\"Downloading {url}\"):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()  # raise error if request fails\n",
    "\n",
    "        ensure_dir(dest_path.parent)\n",
    "        with open(dest_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    logger.info(\"Downloaded %s → %s\", url, dest_path)\n",
    "    return str(dest_path)\n",
    "\n",
    "\n",
    "download_file(\n",
    "    url=\"https://cct-ds-code-challenge-input-data.s3.af-south-1.amazonaws.com/ds_code_challenge_creds.json\",\n",
    "    save_as=\"credentials.json\"\n",
    ")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "DEST_ROOT = \"../data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19865cec",
   "metadata": {},
   "source": [
    "Config set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97434fe2",
   "metadata": {},
   "source": [
    "Functions to load credentials and boto client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_path(rel: str | None, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if rel:\n",
    "        return str(Path(rel) / filename)\n",
    "    return str(Path(DEST_ROOT) / filename)\n",
    "\n",
    "def load_creds_from_file(credentials_path: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Reads {\"s3\": {\"access_key\": \"...\", \"secret_key\": \"...\"}} -> (ak, sk) or (None, None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(credentials_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        ak = data.get(\"s3\", {}).get(\"access_key\")\n",
    "        sk = data.get(\"s3\", {}).get(\"secret_key\")\n",
    "        if ak and sk:\n",
    "            logger.info(\"Loaded S3 credentials from %s\", credentials_path)\n",
    "            return ak, sk\n",
    "        logger.info(\"Credentials file present but missing keys; falling back to env/role.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"No credentials file at %s; falling back to env/role.\", credentials_path)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Failed reading credentials file: %s (falling back to env/role)\", e)\n",
    "    return None, None\n",
    "\n",
    "def make_s3_client(\n",
    "    *, region: str,\n",
    "    credentials_path: str | None = None,\n",
    "    addressing_style: str = \"virtual\",\n",
    "    botocore_extra: Optional[dict] = None,\n",
    "):\n",
    "    ak = sk = None\n",
    "    if credentials_path:\n",
    "        ak, sk = load_creds_from_file(credentials_path)\n",
    "\n",
    "    user_cfg = (botocore_extra or {}).get(\"config\")\n",
    "    config = user_cfg or Config(s3={\"addressing_style\": addressing_style})\n",
    "\n",
    "    client_kwargs = {\"region_name\": region, \"config\": config}\n",
    "    if ak and sk:\n",
    "        client_kwargs.update(aws_access_key_id=ak, aws_secret_access_key=sk)\n",
    "    if botocore_extra:\n",
    "        client_kwargs.update({k: v for k, v in botocore_extra.items() if k != \"config\"})\n",
    "\n",
    "    return boto3.client(\"s3\", **client_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b5e01",
   "metadata": {},
   "source": [
    "S3 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_select_to_file(\n",
    "    *, s3, bucket: str, key: str, sql: str, out_path: str,\n",
    "    input_is_json_document: bool = True,\n",
    "    output_record_delimiter: str = \"\\n\",\n",
    "    log_stats: bool = True,\n",
    "):\n",
    "    input_ser  = {\"JSON\": {\"Type\": \"DOCUMENT\"}} if input_is_json_document else {}\n",
    "    output_ser = {\"JSON\": {\"RecordDelimiter\": output_record_delimiter}}\n",
    "    delim = output_record_delimiter.encode(\"utf-8\")\n",
    "\n",
    "    with timed(\"S3 Select open\"):\n",
    "        try:\n",
    "            resp = s3.select_object_content(\n",
    "                Bucket=bucket, Key=key,\n",
    "                ExpressionType=\"SQL\", Expression=sql,\n",
    "                InputSerialization=input_ser,\n",
    "                OutputSerialization=output_ser,\n",
    "            )\n",
    "        except ClientError as e:\n",
    "            raise RuntimeError(f\"S3 Select failed {client_error_msg(e)}\") from e\n",
    "\n",
    "    ensure_dir(parent_dir_of(out_path))\n",
    "    start = time.perf_counter()\n",
    "    events = rows = 0\n",
    "    tail = b\"\"\n",
    "    bytes_returned = None\n",
    "\n",
    "    with open(out_path, \"wb\", buffering=8 * 1024 * 1024) as fout:\n",
    "        for event in resp[\"Payload\"]:\n",
    "            events += 1\n",
    "            rec = event.get(\"Records\")\n",
    "            if rec:\n",
    "                data = tail + rec[\"Payload\"]\n",
    "                last = data.rfind(delim)\n",
    "                if last != -1:\n",
    "                    fout.write(memoryview(data)[: last + len(delim)])\n",
    "                    rows += data.count(delim, 0, last + 1)\n",
    "                    tail = data[last + len(delim):]\n",
    "                else:\n",
    "                    tail = data\n",
    "            elif \"Stats\" in event and log_stats:\n",
    "                d = event[\"Stats\"][\"Details\"]\n",
    "                bytes_returned = d.get(\"BytesReturned\", bytes_returned)\n",
    "                logger.info(\n",
    "                    \"Stats: scanned=%s processed=%s returned=%s\",\n",
    "                    fmt_bytes(d.get(\"BytesScanned\", 0)),\n",
    "                    fmt_bytes(d.get(\"BytesProcessed\", 0)),\n",
    "                    fmt_bytes(d.get(\"BytesReturned\", 0)),\n",
    "                )\n",
    "            if events % 200 == 0:\n",
    "                resource_snapshot(f\"s3select events={events}\")\n",
    "\n",
    "        if tail:\n",
    "            if not tail.endswith(delim):\n",
    "                fout.write(tail + delim); rows += 1\n",
    "            else:\n",
    "                fout.write(tail); rows += tail.count(delim)\n",
    "\n",
    "    wall = time.perf_counter() - start\n",
    "    logger.info(\"S3 Select complete | events=%d rows~=%.0f time=%.3fs out=%s\",\n",
    "                events, rows, wall, out_path)\n",
    "    return {\"events\": events, \"rows_est\": rows, \"bytes_returned\": bytes_returned,\n",
    "            \"wall_time_s\": wall, \"out_path\": out_path}\n",
    "\n",
    "\n",
    "def download_unpack_s3_files(\n",
    "    *, s3, bucket: str, keys: Iterable[str],\n",
    "    dest_root: str | None = None,\n",
    "    overwrite: bool = False,\n",
    "    preserve_structure: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Download specific S3 keys into a local folder.\n",
    "    If a key ends with plain '.gz' (not '.tar.gz'), decompress it to the same\n",
    "    path minus the '.gz' suffix, overwriting if it exists.\n",
    "\n",
    "    Returns a dict with lists of downloaded, skipped, and error items.\n",
    "    \"\"\"\n",
    "    import os, gzip, shutil, logging\n",
    "    from pathlib import Path\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    root = Path(dest_root or DEST_ROOT)\n",
    "    ensure_dir(root)\n",
    "\n",
    "    downloaded, skipped, errors = [], [], []\n",
    "\n",
    "    logger.info(\"Starting S3 download | bucket=%s keys=%d dest_root=%s overwrite=%s preserve_structure=%s\",\n",
    "                bucket, len(keys), root, overwrite, preserve_structure)\n",
    "\n",
    "    for key in keys:\n",
    "        local_rel = key if preserve_structure else os.path.basename(key)\n",
    "        local_path = root / local_rel\n",
    "        ensure_dir(local_path.parent)\n",
    "\n",
    "        if local_path.exists() and not overwrite:\n",
    "            skipped.append({\"key\": key, \"path\": str(local_path), \"reason\": \"exists\"})\n",
    "            logger.info(\"Skipping (exists): s3://%s/%s -> %s\", bucket, key, local_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Downloading: s3://%s/%s -> %s\", bucket, key, local_path)\n",
    "            s3.download_file(bucket, key, str(local_path))\n",
    "            downloaded.append({\"key\": key, \"path\": str(local_path)})\n",
    "        except Exception as e:\n",
    "            msg = client_error_msg(e)\n",
    "            logger.error(\"Download failed: s3://%s/%s -> %s | %s\", bucket, key, local_path, msg)\n",
    "            errors.append({\"key\": key, \"error\": msg})\n",
    "            continue\n",
    "\n",
    "\n",
    "        lp = local_path.name.lower()\n",
    "        if lp.endswith(\".gz\") and not lp.endswith(\".tar.gz\"):\n",
    "            target_path = local_path.with_suffix(\"\") \n",
    "            try:\n",
    "                logger.info(\"Decompressing: %s -> %s\", local_path, target_path)\n",
    "                gunzip_file(str(local_path), str(target_path))\n",
    "            except Exception as e:\n",
    "                logger.error(\"Decompression failed for %s: %s\", local_path, e)\n",
    "                errors.append({\"key\": key, \"error\": f\"gunzip: {e}\"})\n",
    "\n",
    "    logger.info(\"S3 download complete | downloaded=%d skipped=%d errors=%d dest_root=%s\",\n",
    "                len(downloaded), len(skipped), len(errors), root)\n",
    "\n",
    "    return {\n",
    "        \"downloaded\": downloaded,\n",
    "        \"skipped\": skipped,\n",
    "        \"errors\": errors,\n",
    "        \"dest_root\": str(root),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d656f",
   "metadata": {},
   "source": [
    "Geopanda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944143e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gpd_from_jsonl(\n",
    "    path: str | None = None,\n",
    "    *,\n",
    "    filename: str = \"hex8_features.jsonl\",\n",
    "    dest_root: str | None = None,\n",
    "    crs: str = \"EPSG:4326\",\n",
    "    sep: str = \".\",\n",
    "    sample_logs: int = 3,\n",
    "    log_every_n: int = 500,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Load a GeoDataFrame from JSON Lines where each line is one record in any of:\n",
    "      • Feature: {\"type\":\"Feature\",\"properties\":{...},\"geometry\":{...}}\n",
    "      • Wrapped: {\"s\": {Feature...}} or any single-key wrapper containing a Feature\n",
    "      • Columnar: {\"_1\": properties, \"_2\": geometry}\n",
    "\n",
    "    Resolution order for the input:\n",
    "      1) If `path` is provided, use it.\n",
    "      2) Else use (dest_root or DEST_ROOT)/filename.\n",
    "      3) Else try ./filename.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame (CRS=`crs`) with shapely geometry and flattened properties.\n",
    "    Logs INFO timings and DEBUG progress/resource snapshots (if DEBUG enabled).\n",
    "    \"\"\"\n",
    "    import json, logging, os, time\n",
    "    from pathlib import Path\n",
    "    from typing import Any\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import shape\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # --- resolve input path using provided dest_root or global DEST_ROOT fallback ---\n",
    "    DEST_ROOT_FALLBACK = Path(globals().get(\"DEST_ROOT\", \"../data\"))\n",
    "    candidates: list[Path] = []\n",
    "    if path:\n",
    "        candidates.append(Path(path))\n",
    "    else:\n",
    "        root = Path(dest_root) if dest_root else DEST_ROOT_FALLBACK\n",
    "        candidates.append(root / filename)\n",
    "        candidates.append(Path(filename))  # local fallback\n",
    "\n",
    "    resolved: str | None = None\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            resolved = str(p)\n",
    "            break\n",
    "    if not resolved:\n",
    "        raise FileNotFoundError(f\"Could not find {filename}. Tried: {', '.join(map(str, candidates))}\")\n",
    "\n",
    "    # --- optional file size for context ---\n",
    "    try:\n",
    "        sz = Path(resolved).stat().st_size\n",
    "        logger.debug(\"Input JSONL: %s (%s)\", resolved, fmt_bytes(sz))\n",
    "    except Exception:\n",
    "        logger.debug(\"Input JSONL: %s\", resolved)\n",
    "\n",
    "    # --- timers ---\n",
    "    t0_wall = time.perf_counter()\n",
    "    t0_cpu = time.process_time()\n",
    "\n",
    "    resource_snapshot(\"gpd_from_jsonl:start\")\n",
    "\n",
    "    feats: list[dict[str, Any]] = []\n",
    "    wrapped = colstyle = skipped = json_errors = 0\n",
    "    n_lines = 0\n",
    "    last_log_wall = time.perf_counter()\n",
    "\n",
    "    # --- read & parse lines ---\n",
    "    with open(resolved, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln, raw in enumerate(f, 1):\n",
    "            n_lines += 1\n",
    "            s = raw.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                obj = json.loads(s)\n",
    "            except Exception as e:\n",
    "                json_errors += 1\n",
    "                if logger.isEnabledFor(logging.DEBUG) and json_errors <= sample_logs:\n",
    "                    logger.debug(\"Line %d: JSON decode error: %s\", ln, e)\n",
    "                continue\n",
    "\n",
    "            rec = None\n",
    "            # Direct Feature\n",
    "            if isinstance(obj, dict) and obj.get(\"type\") == \"Feature\" and \"geometry\" in obj and \"properties\" in obj:\n",
    "                rec = obj\n",
    "            # Known wrappers\n",
    "            elif isinstance(obj, dict) and \"s\" in obj and isinstance(obj[\"s\"], dict):\n",
    "                rec = obj[\"s\"]; wrapped += 1\n",
    "            elif isinstance(obj, dict) and \"_1\" in obj and \"_2\" in obj:\n",
    "                rec = {\"type\": \"Feature\", \"properties\": obj[\"_1\"], \"geometry\": obj[\"_2\"]}; colstyle += 1\n",
    "            else:\n",
    "                # Any single-key wrapper with a Feature-like payload\n",
    "                if isinstance(obj, dict):\n",
    "                    for v in obj.values():\n",
    "                        if isinstance(v, dict) and \"geometry\" in v and \"properties\" in v:\n",
    "                            rec = v; wrapped += 1\n",
    "                            break\n",
    "\n",
    "            if not (isinstance(rec, dict) and \"geometry\" in rec and \"properties\" in rec):\n",
    "                skipped += 1\n",
    "                if logger.isEnabledFor(logging.DEBUG) and skipped <= sample_logs:\n",
    "                    logger.debug(\"Line %d: no usable 'geometry'/'properties' wrapper found\", ln)\n",
    "                continue\n",
    "\n",
    "            feats.append(rec)\n",
    "\n",
    "            # periodic DEBUG progress & resources\n",
    "            if logger.isEnabledFor(logging.DEBUG) and (n_lines % log_every_n == 0):\n",
    "                now = time.perf_counter()\n",
    "                rate = log_every_n / max(1e-9, (now - last_log_wall))\n",
    "                last_log_wall = now\n",
    "                logger.debug(\"Progress: %d lines, %d features (%.0f l/s)\", n_lines, len(feats), rate)\n",
    "                resource_snapshot(f\"gpd_from_jsonl:line={n_lines}\")\n",
    "\n",
    "    # --- build GeoDataFrame ---\n",
    "    t1 = time.perf_counter()\n",
    "    props = [feat.get(\"properties\", {}) for feat in feats]\n",
    "    geoms = []\n",
    "    none_geom = 0\n",
    "    for feat in feats:\n",
    "        g = feat.get(\"geometry\")\n",
    "        if g is None:\n",
    "            geoms.append(None); none_geom += 1\n",
    "        else:\n",
    "            try:\n",
    "                geoms.append(shape(g))\n",
    "            except Exception:\n",
    "                geoms.append(None); none_geom += 1\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    df_props = pd.json_normalize(props, sep=sep)\n",
    "    t3 = time.perf_counter()\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(df_props, geometry=geoms, crs=crs)\n",
    "    t4 = time.perf_counter()\n",
    "\n",
    "    resource_snapshot(\"gpd_from_jsonl:end-parse+build\")\n",
    "\n",
    "    # --- timings ---\n",
    "    wall_total = time.perf_counter() - t0_wall\n",
    "    cpu_total = time.process_time() - t0_cpu\n",
    "    parse_wall = t1 - t0_wall\n",
    "    geom_wall  = t2 - t1\n",
    "    norm_wall  = t3 - t2\n",
    "    gdf_wall   = t4 - t3\n",
    "\n",
    "    logger.info(\n",
    "        \"Loaded %d features from %s | wall=%.3fs cpu=%.3fs | parse=%.3fs geom=%.3fs normalize=%.3fs geodf=%.3fs | \"\n",
    "        \"wrapped=%d columnar=%d skipped=%d json_errors=%d none_geom=%d\",\n",
    "        len(feats), resolved, wall_total, cpu_total, parse_wall, geom_wall, norm_wall, gdf_wall,\n",
    "        wrapped, colstyle, skipped, json_errors, none_geom\n",
    "    )\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def _ensure_key_col(gdf: gpd.GeoDataFrame, key: str = \"index\") -> gpd.GeoDataFrame:\n",
    "    return gdf if key in gdf.columns else gdf.reset_index().rename(columns={\"index\": key})\n",
    "\n",
    "def compare_hex_gdfs_simple(\n",
    "    left: gpd.GeoDataFrame,\n",
    "    right: gpd.GeoDataFrame,\n",
    "    key: str = \"index\",\n",
    "    geom_tolerance: float = 0.0,  \n",
    "    na_equal: bool = True,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "\n",
    "    left  = _ensure_key_col(left, key)\n",
    "    right = _ensure_key_col(right, key)\n",
    "\n",
    "    required = {key, \"centroid_lat\", \"centroid_lon\", \"geometry\"}\n",
    "    for name, df in ((\"left\", left), (\"right\", right)):\n",
    "        missing = sorted(required - set(df.columns))\n",
    "        if missing:\n",
    "            raise ValueError(f\"{name} GeoDataFrame missing columns: {missing}\")\n",
    "\n",
    "\n",
    "    geom_l = left.geometry.name\n",
    "    geom_r = right.geometry.name\n",
    "    L = left[[key, \"centroid_lat\", \"centroid_lon\", geom_l]].copy()\n",
    "    R = right[[key, \"centroid_lat\", \"centroid_lon\", geom_r]].copy()\n",
    "\n",
    "    # Align CRS (reproject right -> left if both set and differ)\n",
    "    if getattr(left, \"crs\", None) and getattr(right, \"crs\", None) and left.crs != right.crs:\n",
    "        R = gpd.GeoDataFrame(R, geometry=geom_r, crs=right.crs).to_crs(left.crs)\n",
    "\n",
    "    # Merge on key\n",
    "    m = L.merge(R, on=key, how=\"outer\", suffixes=(\"_l\", \"_r\"), indicator=True)\n",
    "\n",
    "    only_in_left  = m.loc[m[\"_merge\"] == \"left_only\",  [key]].reset_index(drop=True)\n",
    "    only_in_right = m.loc[m[\"_merge\"] == \"right_only\", [key]].reset_index(drop=True)\n",
    "    both = m.loc[m[\"_merge\"] == \"both\"].copy()\n",
    "\n",
    "    # Element-wise attr equality\n",
    "    def _eq(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "        out = (a == b)\n",
    "        return out | (a.isna() & b.isna()) if na_equal else out\n",
    "\n",
    "    lat_eq = _eq(both[\"centroid_lat_l\"], both[\"centroid_lat_r\"])\n",
    "    lon_eq = _eq(both[\"centroid_lon_l\"], both[\"centroid_lon_r\"])\n",
    "\n",
    "    # Element-wise geometry equality\n",
    "    g1 = gpd.GeoSeries(both[f\"{geom_l}_l\"], crs=left.crs)\n",
    "    g2 = gpd.GeoSeries(both[f\"{geom_r}_r\"], crs=left.crs)  # already reprojected if needed\n",
    "\n",
    "    if geom_tolerance <= 0:\n",
    "        geom_eq = g1.geom_equals(g2)\n",
    "        if na_equal:\n",
    "            geom_eq = geom_eq | (g1.isna() & g2.isna())\n",
    "    else:\n",
    "        def _within(a: BaseGeometry, b: BaseGeometry) -> bool:\n",
    "            if a is None or b is None:\n",
    "                return na_equal and (a is None and b is None)\n",
    "            try:\n",
    "                return a.distance(b) <= geom_tolerance\n",
    "            except Exception:\n",
    "                return False\n",
    "        geom_eq = pd.Series([_within(a, b) for a, b in zip(g1.values, g2.values)], index=both.index)\n",
    "\n",
    "\n",
    "    all_eq = lat_eq & lon_eq & geom_eq\n",
    "    matches = both.loc[all_eq, [key]].reset_index(drop=True)\n",
    "    diffs   = both.loc[~all_eq].copy()\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    if (~lat_eq).any():\n",
    "        t = diffs.loc[diffs.index.intersection(both.index[~lat_eq]), [key, \"centroid_lat_l\", \"centroid_lat_r\"]].copy()\n",
    "        t.insert(1, \"column\", \"centroid_lat\")\n",
    "        t.rename(columns={\"centroid_lat_l\": \"left\", \"centroid_lat_r\": \"right\"}, inplace=True)\n",
    "        rows.append(t)\n",
    "    if (~lon_eq).any():\n",
    "        t = diffs.loc[diffs.index.intersection(both.index[~lon_eq]), [key, \"centroid_lon_l\", \"centroid_lon_r\"]].copy()\n",
    "        t.insert(1, \"column\", \"centroid_lon\")\n",
    "        t.rename(columns={\"centroid_lon_l\": \"left\", \"centroid_lon_r\": \"right\"}, inplace=True)\n",
    "        rows.append(t)\n",
    "    if (~geom_eq).any():\n",
    "        t = diffs.loc[diffs.index.intersection(both.index[~geom_eq]), [key, f\"{geom_l}_l\", f\"{geom_r}_r\"]].copy()\n",
    "        t.insert(1, \"column\", \"geometry\")\n",
    "        t[\"left\"]  = gpd.GeoSeries(t.pop(f\"{geom_l}_l\"), crs=left.crs).to_wkt()\n",
    "        t[\"right\"] = gpd.GeoSeries(t.pop(f\"{geom_r}_r\"), crs=left.crs).to_wkt()\n",
    "        rows.append(t[[key, \"column\", \"left\", \"right\"]])\n",
    "\n",
    "    mismatches_long = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[key, \"column\", \"left\", \"right\"])\n",
    "\n",
    "\n",
    "    if not diffs.empty:\n",
    "        keep = [key, \"centroid_lat_l\", \"centroid_lat_r\", \"centroid_lon_l\", \"centroid_lon_r\", f\"{geom_l}_l\", f\"{geom_r}_r\"]\n",
    "        mismatches_wide = diffs[[c for c in keep if c in diffs.columns]].reset_index(drop=True)\n",
    "    else:\n",
    "        mismatches_wide = pd.DataFrame(columns=[key])\n",
    "\n",
    "    return {\n",
    "        \"only_in_left\": only_in_left,\n",
    "        \"only_in_right\": only_in_right,\n",
    "        \"matches\": matches,\n",
    "        \"mismatches_long\": mismatches_long,\n",
    "        \"mismatches_wide\": mismatches_wide,\n",
    "    }\n",
    "import logging\n",
    "\n",
    "def validate_hex_gdf(gdf):\n",
    "    \"\"\"\n",
    "    Run data quality checks on a GeoDataFrame of H3 polygons.\n",
    "\n",
    "    Expected columns:\n",
    "        - index (H3 index)\n",
    "        - centroid_lat\n",
    "        - centroid_lon\n",
    "        - resolution\n",
    "        - geometry\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(\"Running data quality checks...\")\n",
    "    results = {}\n",
    "\n",
    "    # --- 1) Missing values ---\n",
    "    n_missing = gdf.isna().sum()\n",
    "    results[\"missing_values\"] = n_missing.to_dict()\n",
    "    if n_missing.sum() > 0:\n",
    "        logging.warning(\" Found NaNs:\\n%s\", n_missing[n_missing > 0])\n",
    "    else:\n",
    "        logging.info(\" No missing values detected.\")\n",
    "\n",
    "    # --- 2) Resolution consistency ---\n",
    "    bad_res = gdf[gdf[\"resolution\"] != 8]\n",
    "    results[\"bad_resolution_count\"] = len(bad_res)\n",
    "    if not bad_res.empty:\n",
    "        logging.warning(\" Rows with resolution != 8: %d\", len(bad_res))\n",
    "    else:\n",
    "        logging.info(\" All rows have resolution = 8.\")\n",
    "\n",
    "    # --- 3) Geometry validity ---\n",
    "    invalid_geom = gdf[~gdf.is_valid]\n",
    "    results[\"invalid_geometry_count\"] = len(invalid_geom)\n",
    "    if not invalid_geom.empty:\n",
    "        logging.warning(\" Invalid geometries found: %d\", len(invalid_geom))\n",
    "    else:\n",
    "        logging.info(\" All geometries are valid.\")\n",
    "\n",
    "    # --- 4) Centroid plausibility ---\n",
    "    lat_out_of_range = gdf[(gdf[\"centroid_lat\"] < -90) | (gdf[\"centroid_lat\"] > 90)]\n",
    "    lon_out_of_range = gdf[(gdf[\"centroid_lon\"] < -180) | (gdf[\"centroid_lon\"] > 180)]\n",
    "    results[\"bad_lat_count\"] = len(lat_out_of_range)\n",
    "    results[\"bad_lon_count\"] = len(lon_out_of_range)\n",
    "\n",
    "    if results[\"bad_lat_count\"] > 0 or results[\"bad_lon_count\"] > 0:\n",
    "        logging.warning(\n",
    "            \" Centroids out of bounds: lat=%d, lon=%d\",\n",
    "            results[\"bad_lat_count\"],\n",
    "            results[\"bad_lon_count\"],\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\" All centroids are within valid lat/lon ranges.\")\n",
    "\n",
    "    # --- 5) Duplicate H3 indexes ---\n",
    "    dupes = gdf[\"index\"].duplicated().sum()\n",
    "    results[\"duplicate_index_count\"] = int(dupes)\n",
    "    if dupes > 0:\n",
    "        logging.warning(\" Duplicate H3 indexes found: %d\", dupes)\n",
    "    else:\n",
    "        logging.info(\" No duplicate H3 indexes.\")\n",
    "\n",
    "    # --- Summary ---\n",
    "    logging.info(\" Data quality check complete.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a186b37",
   "metadata": {},
   "source": [
    "local file loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_project_files(\n",
    "    file_map: dict[str, str],\n",
    "    project_root_name: str = \"ds_code_challenge\",\n",
    "    inject_globals: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load files from ROOT/data based on a {file_name: variable_name} map.\n",
    "\n",
    "    Supports:\n",
    "      - .csv / .csv.gz  -> pandas DataFrame\n",
    "      - .geojson / .geojson.gz -> GeoDataFrame\n",
    "      - .ods -> Excel (odf engine)\n",
    "    \"\"\"\n",
    "    with timed(\"load_project_files (resolve ROOT/DATA_DIR)\"):\n",
    "        ROOT = Path(__file__).resolve().parents[0] if \"__file__\" in globals() else Path().resolve()\n",
    "        while ROOT.name != project_root_name and ROOT.parent != ROOT:\n",
    "            ROOT = ROOT.parent\n",
    "        DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "    results: dict[str, pd.DataFrame | gpd.GeoDataFrame] = {}\n",
    "    resource_snapshot(\"load_project_files:start\")\n",
    "\n",
    "    for file_name, var_name in file_map.items():\n",
    "        file_path = DATA_DIR / file_name\n",
    "        logger.info(\"Processing %s...\", file_path)\n",
    "\n",
    "        with timed(f\"read:{file_path.name}\"):\n",
    "            suffix = \"\".join(file_path.suffixes).lower()\n",
    "\n",
    "            if suffix.endswith((\".csv\", \".csv.gz\")):\n",
    "                df = pd.read_csv(file_path)\n",
    "                results[var_name] = df\n",
    "                logger.info(\"→ %s loaded as DataFrame shape=%s\", var_name, getattr(df, \"shape\", None))\n",
    "\n",
    "            elif suffix.endswith((\".geojson\", \".geojson.gz\")):\n",
    "                gdf = gpd.read_file(file_path)\n",
    "                results[var_name] = gdf\n",
    "                logger.info(\"→ %s loaded as GeoDataFrame len=%d\", var_name, len(gdf))\n",
    "\n",
    "            elif suffix.endswith(\".ods\"):\n",
    "                df = pd.read_excel(file_path, engine=\"odf\")\n",
    "                results[var_name] = df\n",
    "                logger.info(\"→ %s loaded from ODS shape=%s\", var_name, getattr(df, \"shape\", None))\n",
    "\n",
    "            else:\n",
    "                logger.warning(\"Skipping unsupported file type: %s\", file_path)\n",
    "                continue\n",
    "\n",
    "            if inject_globals:\n",
    "                globals()[var_name] = results[var_name]\n",
    "\n",
    "            resource_snapshot(f\"after_load:{file_path.name}\")\n",
    "\n",
    "    logger.info(\"All files loaded successfully.\")\n",
    "    resource_snapshot(\"load_project_files:end\")\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"af-south-1\"\n",
    "BUCKET = \"cct-ds-code-challenge-input-data\"\n",
    "FILE_NAME    = \"city-hex-polygons-8-10.geojson\"\n",
    "OUT_JSONL = \"../data/hex8_features.jsonl\"  \n",
    "CREDENTIALS_PATH = \"credentials.json\"\n",
    "\n",
    "ADDRESSING_STYLE = \"virtual\"\n",
    "\n",
    "s3 = make_s3_client(region=REGION, credentials_path=CREDENTIALS_PATH, addressing_style=ADDRESSING_STYLE)\n",
    "\n",
    "\n",
    "SQL = (\n",
    "    \"\"\"\n",
    "    SELECT s\n",
    "    FROM S3Object[*].features[*] s\n",
    "    WHERE s.properties.resolution = 8\n",
    "    \"\"\"\n",
    ")\n",
    "select_summary = s3_select_to_file(\n",
    "    s3=s3,\n",
    "    bucket=BUCKET,\n",
    "    key=FILE_NAME,\n",
    "    sql=SQL,\n",
    "    out_path=OUT_JSONL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_TO_DOWNLOAD = [\n",
    "    \"city-hex-polygons-8.geojson\",\n",
    "    \"city-hex-polygons-8-10.geojson\",\n",
    "    \"sr.csv.gz\",\n",
    "    \"sr_hex_truncated.csv\",\n",
    "    \"sr_hex.csv.gz\"\n",
    "]\n",
    "\n",
    "download_summary = download_unpack_s3_files(\n",
    "    s3=s3,\n",
    "    bucket=BUCKET,\n",
    "    keys=FILES_TO_DOWNLOAD,\n",
    "    dest_root=DEST_ROOT,\n",
    "    overwrite=False,\n",
    "    preserve_structure=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00164f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_map = {\n",
    "    \"city-hex-polygons-8.geojson\": \"gdf_city_hex_8\"\n",
    "}\n",
    "\n",
    "load_project_files(file_map)\n",
    "\n",
    "\n",
    "gdf = gpd_from_jsonl()\n",
    "res = compare_hex_gdfs_simple(gdf_city_hex_8, gdf, key=\"index\", geom_tolerance=0.0) \n",
    "\n",
    "print(\"Only in left:\", len(res[\"only_in_left\"]))\n",
    "print(\"Only in right:\", len(res[\"only_in_right\"]))\n",
    "print(\"Matches:\", len(res[\"matches\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_results = validate_hex_gdf(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978dccd9",
   "metadata": {},
   "source": [
    "# Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc887ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -q\n",
    "\n",
    "\n",
    "# --- Configure logging for the tests ---\n",
    "logger = logging.getLogger(__name__)\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s | %(message)s\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def _write_text(p: Path, s: str):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(s, encoding=\"utf-8\")\n",
    "\n",
    "def _gz_write_bytes(p: Path, b: bytes):\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with gzip.open(p, \"wb\") as f:\n",
    "        f.write(b)\n",
    "\n",
    "# ---------- Tests ----------\n",
    "\n",
    "def test_ensure_dir_and_parent_dir_of(tmp_path: Path, caplog):\n",
    "    logging.info(\"TEST: ensure_dir / parent_dir_of\")\n",
    "    d = tmp_path / \"a\" / \"b\"\n",
    "    ensure_dir(str(d))\n",
    "    assert d.exists() and d.is_dir()\n",
    "    assert parent_dir_of(str(d / \"file.txt\")).endswith(os.sep.join([\"a\",\"b\"]))\n",
    "\n",
    "@pytest.mark.parametrize(\"val,expected\", [\n",
    "    (0, \"0.0B\"),\n",
    "    (1023, \"1023.0B\"),\n",
    "    (1024, \"1.0KB\"),\n",
    "    (1024**2, \"1.0MB\"),\n",
    "    (\"oops\", \"oops\"),\n",
    "])\n",
    "def test_fmt_bytes(val, expected):\n",
    "    logging.info(\"TEST: fmt_bytes\")\n",
    "    assert fmt_bytes(val) == expected\n",
    "\n",
    "def test_gunzip_file_roundtrip(tmp_path: Path):\n",
    "    logging.info(\"TEST: gunzip_file\")\n",
    "    src_gz = tmp_path / \"data.txt.gz\"\n",
    "    dst = tmp_path / \"out.txt\"\n",
    "    payload = b\"hello gunzip\\n\"\n",
    "    _gz_write_bytes(src_gz, payload)\n",
    "    gunzip_file(str(src_gz), str(dst))\n",
    "    assert dst.read_bytes() == payload\n",
    "\n",
    "def test_client_error_msg_happy_path():\n",
    "    logging.info(\"TEST: client_error_msg\")\n",
    "    class E:\n",
    "        response = {\"Error\": {\"Code\": \"NoSuchKey\", \"Message\": \"missing\"}}\n",
    "    assert client_error_msg(E()) == \"[NoSuchKey] missing\"\n",
    "\n",
    "def test_client_error_msg_fallback():\n",
    "    logging.info(\"TEST: client_error_msg (fallback)\")\n",
    "    class E: pass\n",
    "    e = E()\n",
    "    assert client_error_msg(e) == str(e)\n",
    "\n",
    "def test_timed_logs(caplog):\n",
    "    logging.info(\"TEST: timed\")\n",
    "    caplog.set_level(logging.INFO)\n",
    "    with timed(\"quick task\", enabled=True):\n",
    "        time.sleep(0.01)\n",
    "    assert any(\"quick task took\" in rec.getMessage() for rec in caplog.records)\n",
    "\n",
    "def test_resource_snapshot_doesnt_crash_and_debugs(caplog):\n",
    "    logging.info(\"TEST: resource_snapshot\")\n",
    "    caplog.set_level(logging.DEBUG)\n",
    "\n",
    "    # Inject fakes into the function's globals so it can \"see\" psutil/tracemalloc\n",
    "    fn_g = resource_snapshot.__globals__\n",
    "    class _P:\n",
    "        class _Proc:\n",
    "            def __init__(self, *_): pass\n",
    "            def memory_info(self):\n",
    "                class M: rss = 123456\n",
    "                return M()\n",
    "            def cpu_percent(self, interval=0.0): return 1.23\n",
    "        def Process(self, pid): return self._Proc()\n",
    "    class _T:\n",
    "        def is_tracing(self): return True\n",
    "        def get_traced_memory(self): return (1000, 2000)\n",
    "    fn_g[\"_psutil\"] = _P()\n",
    "    fn_g[\"_tracemalloc\"] = _T()\n",
    "\n",
    "    resource_snapshot(\"unit\")\n",
    "    # Should log DEBUG with \"RES[unit]\"\n",
    "    assert any(\"RES[unit]\" in rec.getMessage() for rec in caplog.records if rec.levelno == logging.DEBUG)\n",
    "\n",
    "def test_resolve_path_uses_rel_or_DEST_ROOT(tmp_path: Path, monkeypatch):\n",
    "    logging.info(\"TEST: resolve_path\")\n",
    "    # with rel\n",
    "    p = resolve_path(str(tmp_path), \"file.txt\")\n",
    "    assert p == str(tmp_path / \"file.txt\")\n",
    "    # without rel -> uses DEST_ROOT\n",
    "    fn_g = resolve_path.__globals__\n",
    "    fn_g[\"DEST_ROOT\"] = str(tmp_path / \"root\")\n",
    "    q = resolve_path(None, \"x.bin\")\n",
    "    assert q == str((tmp_path / \"root\" / \"x.bin\"))\n",
    "\n",
    "def test_load_creds_from_file_ok(tmp_path: Path, caplog):\n",
    "    logging.info(\"TEST: load_creds_from_file (ok)\")\n",
    "    caplog.set_level(logging.INFO)\n",
    "    creds = {\"s3\": {\"access_key\": \"AK\", \"secret_key\": \"SK\"}}\n",
    "    p = tmp_path / \"credentials.json\"\n",
    "    _write_text(p, json.dumps(creds))\n",
    "    ak, sk = load_creds_from_file(str(p))\n",
    "    assert (ak, sk) == (\"AK\", \"SK\")\n",
    "    assert any(\"Loaded S3 credentials\" in rec.getMessage() for rec in caplog.records)\n",
    "\n",
    "def test_load_creds_from_file_missing(tmp_path: Path, caplog):\n",
    "    logging.info(\"TEST: load_creds_from_file (missing)\")\n",
    "    caplog.set_level(logging.INFO)\n",
    "    ak, sk = load_creds_from_file(str(tmp_path / \"nope.json\"))\n",
    "    assert (ak, sk) == (None, None)\n",
    "    assert any(\"No credentials file\" in rec.getMessage() for rec in caplog.records)\n",
    "\n",
    "def test_make_s3_client_injects_kwargs(monkeypatch):\n",
    "    logging.info(\"TEST: make_s3_client\")\n",
    "    calls = {}\n",
    "    def fake_client(name, **kwargs):\n",
    "        calls[\"service\"] = name\n",
    "        calls[\"kwargs\"] = kwargs\n",
    "        class D: pass\n",
    "        return D()\n",
    "    monkeypatch.setattr(\"boto3.client\", fake_client)\n",
    "    # also provide a fake Config type if the real one isn't present\n",
    "    if \"Config\" not in make_s3_client.__globals__:\n",
    "        class _C:\n",
    "            def __init__(self, **kw): self.kw = kw\n",
    "        make_s3_client.__globals__[\"Config\"] = _C\n",
    "\n",
    "    c = make_s3_client(region=\"af-south-1\", credentials_path=None, addressing_style=\"virtual\")\n",
    "    assert calls[\"service\"] == \"s3\"\n",
    "    assert calls[\"kwargs\"][\"region_name\"] == \"af-south-1\"\n",
    "    assert \"config\" in calls[\"kwargs\"]\n",
    "\n",
    "def test_s3_select_to_file_writes_records(tmp_path: Path, caplog):\n",
    "    logging.info(\"TEST: s3_select_to_file\")\n",
    "    caplog.set_level(logging.INFO)\n",
    "    class FakeS3:\n",
    "        def select_object_content(self, **kw):\n",
    "            # Emit two record chunks and stats, like AWS does\n",
    "            payload = [\n",
    "                {\"Records\": {\"Payload\": b'{\"a\":1}\\n{\"a\":2}\\n{\"a\":3}'}},\n",
    "                {\"Records\": {\"Payload\": b'\\n{\"a\":4}\\n{\"a\":5}'}},\n",
    "                {\"Stats\": {\"Details\": {\"BytesScanned\": 10, \"BytesProcessed\": 10, \"BytesReturned\": 10}}},\n",
    "                {\"End\": {}},\n",
    "            ]\n",
    "            return {\"Payload\": payload}\n",
    "    out = tmp_path / \"out.jsonl\"\n",
    "    r = s3_select_to_file(\n",
    "        s3=FakeS3(), bucket=\"b\", key=\"k\", sql=\"SELECT * FROM s3object\", out_path=str(out),\n",
    "        input_is_json_document=True, output_record_delimiter=\"\\n\"\n",
    "    )\n",
    "    lines = out.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "    assert len(lines) == 5\n",
    "    assert r[\"rows_est\"] == 5\n",
    "    assert any(\"Stats: scanned=\" in rec.getMessage() for rec in caplog.records)\n",
    "\n",
    "def test_download_unpack_s3_files_downloads_and_gunzips(tmp_path: Path, caplog):\n",
    "    logging.info(\"TEST: download_unpack_s3_files\")\n",
    "    caplog.set_level(logging.INFO)\n",
    "    class FakeS3:\n",
    "        def __init__(self): self.calls=[]\n",
    "        def download_file(self, bucket, key, dest):\n",
    "            self.calls.append((bucket,key,dest))\n",
    "            p = Path(dest); p.parent.mkdir(parents=True, exist_ok=True)\n",
    "            if key.endswith(\".gz\") and not key.endswith(\".tar.gz\"):\n",
    "                _gz_write_bytes(p, b\"gzdata\")\n",
    "            else:\n",
    "                _write_text(p, \"plain\")\n",
    "    keys = [\"a/b/plain.txt\", \"a/c/file.txt.gz\", \"a/d/archive.tar.gz\"]\n",
    "    res = download_unpack_s3_files(s3=FakeS3(), bucket=\"bucket\", keys=keys, dest_root=str(tmp_path))\n",
    "    # Files exist\n",
    "    assert (tmp_path / \"a\" / \"b\" / \"plain.txt\").exists()\n",
    "    assert (tmp_path / \"a\" / \"c\" / \"file.txt\").exists()             # gunzipped\n",
    "    assert (tmp_path / \"a\" / \"d\" / \"archive.tar.gz\").exists()       # not gunzipped due to .tar.gz\n",
    "    assert len(res[\"downloaded\"]) == 3\n",
    "    assert any(\"Decompressing:\" in rec.getMessage() for rec in caplog.records)\n",
    "\n",
    "@pytest.mark.skipif(pytest.importorskip(\"geopandas\", reason=\"geopandas required\") is None, reason=\"geopandas missing\")\n",
    "def test_gpd_from_jsonl_parses_minimal(tmp_path: Path, caplog):\n",
    "    logging.info(\"TEST: gpd_from_jsonl\")\n",
    "    import geopandas as gpd  # noqa: F401\n",
    "    data = [\n",
    "        json.dumps({\"type\":\"Feature\",\"properties\":{\"index\":\"hex1\",\"centroid_lat\":-33.9,\"centroid_lon\":18.6,\"resolution\":8},\"geometry\":{\"type\":\"Point\",\"coordinates\":[18.6,-33.9]}}),\n",
    "        json.dumps({\"s\":{\"type\":\"Feature\",\"properties\":{\"index\":\"hex2\",\"centroid_lat\":-33.8,\"centroid_lon\":18.7,\"resolution\":8},\"geometry\":{\"type\":\"Point\",\"coordinates\":[18.7,-33.8]}}}),\n",
    "        \"not json\",\n",
    "    ]\n",
    "    p = tmp_path / \"hex8_features.jsonl\"\n",
    "    _write_text(p, \"\\n\".join(data))\n",
    "    caplog.set_level(logging.INFO)\n",
    "    gdf = gpd_from_jsonl(path=str(p))\n",
    "    assert {\"index\",\"centroid_lat\",\"centroid_lon\",\"resolution\",\"geometry\"} <= set(gdf.columns)\n",
    "    assert len(gdf) == 2\n",
    "\n",
    "@pytest.mark.skipif(pytest.importorskip(\"geopandas\", reason=\"geopandas required\") is None, reason=\"geopandas missing\")\n",
    "def test_compare_hex_gdfs_simple_matches_and_diffs(tmp_path: Path):\n",
    "    logging.info(\"TEST: compare_hex_gdfs_simple\")\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Polygon\n",
    "\n",
    "    left = gpd.GeoDataFrame(\n",
    "        {\"index\":[\"h1\",\"h2\"], \"centroid_lat\":[-1.0,-2.0], \"centroid_lon\":[10.0,20.0]},\n",
    "        geometry=[Polygon([(0,0),(1,0),(1,1),(0,1)]), Polygon([(2,2),(3,2),(3,3),(2,3)])],\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    right = left.copy()\n",
    "    # Introduce a difference\n",
    "    right.loc[1, \"centroid_lon\"] = 21.0\n",
    "\n",
    "    res = compare_hex_gdfs_simple(left, right, key=\"index\")\n",
    "    assert set(res.keys()) == {\"only_in_left\",\"only_in_right\",\"matches\",\"mismatches_long\",\"mismatches_wide\"}\n",
    "    assert res[\"matches\"][\"index\"].tolist() == [\"h1\"]\n",
    "    assert res[\"mismatches_long\"][\"column\"].tolist() == [\"centroid_lon\"]\n",
    "\n",
    "@pytest.mark.skipif(pytest.importorskip(\"geopandas\", reason=\"geopandas required\") is None, reason=\"geopandas missing\")\n",
    "def test_validate_hex_gdf_flags_ok(tmp_path: Path, caplog):\n",
    "    logging.info(\"TEST: validate_hex_gdf\")\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        {\"index\":[\"x\"], \"centroid_lat\":[-33.9], \"centroid_lon\":[18.6], \"resolution\":[8]},\n",
    "        geometry=[Point(18.6,-33.9)],\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "    caplog.set_level(logging.INFO)\n",
    "    r = validate_hex_gdf(gdf)\n",
    "    assert r[\"bad_resolution_count\"] == 0\n",
    "    assert r[\"invalid_geometry_count\"] == 0\n",
    "    assert r[\"duplicate_index_count\"] == 0\n",
    "\n",
    "def test_load_project_files_reads_csv_and_geojson(tmp_path: Path, monkeypatch, caplog):\n",
    "    logging.info(\"TEST: load_project_files\")\n",
    "    caplog.set_level(logging.INFO)\n",
    "\n",
    "    # Build fake project tree: <tmp>/ds_code_challenge/data/{sr.csv, poly.geojson}\n",
    "    proj = tmp_path / \"ds_code_challenge\"\n",
    "    data = proj / \"data\"\n",
    "    data.mkdir(parents=True)\n",
    "    # Simple CSV\n",
    "    _write_text(data / \"sr.csv\", \"a,b\\n1,2\\n\")\n",
    "    # Minimal GeoJSON FeatureCollection\n",
    "    geojson = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": [\n",
    "            {\"type\":\"Feature\",\"properties\":{\"p\":1},\"geometry\":{\"type\":\"Point\",\"coordinates\":[0,0]}},\n",
    "        ],\n",
    "    }\n",
    "    _write_text(data / \"hex.geojson\", json.dumps(geojson))\n",
    "\n",
    "    # Change CWD into project so Path().resolve() ascends correctly\n",
    "    monkeypatch.chdir(proj)\n",
    "\n",
    "    # geopandas may be required; if unavailable, skip geojson path gracefully by catching ImportError\n",
    "    try:\n",
    "        out = load_project_files({\"sr.csv\":\"df_sr\",\"hex.geojson\":\"df_hex\"})\n",
    "        assert \"df_sr\" in out\n",
    "        # df_hex present only if geopandas installed\n",
    "        if \"df_hex\" in out:\n",
    "            assert hasattr(out[\"df_hex\"], \"geometry\")\n",
    "    except Exception as e:\n",
    "        # If failure is purely due to geopandas import, mark xpass-like\n",
    "        if \"geopandas\" in str(e).lower():\n",
    "            pytest.xfail(\"geopandas not installed; geojson part skipped\")\n",
    "        else:\n",
    "            raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
