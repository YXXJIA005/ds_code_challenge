{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9d17db",
   "metadata": {},
   "source": [
    "## Notebook 3: Further Data Transformation  \n",
    "\n",
    "This notebook third stage of the data engineering challenge\n",
    "It performs three main steps:  \n",
    "1. **Subsampling**: Select service requests near the centroid of Bellville South.  \n",
    "2. **Augmentation**: Enrich requests with wind data from the Bellville South AQM site.  \n",
    "3. **Anonymisation**: Apply spatial and temporal precision limits while removing sensitive identifiers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b09e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# --- Stdlib ---\n",
    "import contextlib\n",
    "import functools\n",
    "import gzip\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "import types\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Optional\n",
    "from pyproj import Transformer\n",
    "\n",
    "# --- Main ---\n",
    "import __main__ as nb\n",
    "\n",
    "# --- Data Science ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- HTTP ---\n",
    "import requests\n",
    "\n",
    "# --- Geo ---\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "\n",
    "# --- Testing ---\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "# --- Optional resources ---\n",
    "try:\n",
    "    import psutil as _psutil\n",
    "except Exception:\n",
    "    _psutil = None\n",
    "\n",
    "try:\n",
    "    import tracemalloc as _tracemalloc\n",
    "except Exception:\n",
    "    _tracemalloc = None\n",
    "\n",
    "# --- Config ---\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# --- Logging setup ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Utilities ---\n",
    "def ensure_dir(path: str | os.PathLike) -> None:\n",
    "    os.makedirs(str(path), exist_ok=True)\n",
    "\n",
    "def parent_dir_of(path: str | os.PathLike) -> str:\n",
    "    return os.path.dirname(str(path)) or \".\"\n",
    "\n",
    "def fmt_bytes(n: int | float) -> str:\n",
    "    try:\n",
    "        n = float(n)\n",
    "    except Exception:\n",
    "        return str(n)\n",
    "    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"):\n",
    "        if abs(n) < 1024 or unit == \"PB\":\n",
    "            return f\"{n:.1f}{unit}\"\n",
    "        n /= 1024\n",
    "\n",
    "def gunzip_file(src: str, dst: str) -> None:\n",
    "    ensure_dir(parent_dir_of(dst))\n",
    "    with gzip.open(src, \"rb\") as fin, open(dst, \"wb\") as fout:\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "\n",
    "def client_error_msg(e) -> str:\n",
    "    code = getattr(e, \"response\", {}).get(\"Error\", {}).get(\"Code\")\n",
    "    msg  = getattr(e, \"response\", {}).get(\"Error\", {}).get(\"Message\")\n",
    "    return f\"[{code}] {msg}\" if code or msg else str(e)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timed(label: str, enabled: bool = True):\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if enabled:\n",
    "            logger.info(\"%s took %.3f s\", label, time.perf_counter() - t0)\n",
    "\n",
    "def resource_snapshot(note: str = \"\") -> None:\n",
    "    if not logger.isEnabledFor(logging.DEBUG):\n",
    "        return\n",
    "    parts: list[str] = []\n",
    "    if _psutil:\n",
    "        try:\n",
    "            p = _psutil.Process(os.getpid())\n",
    "            parts.append(f\"rss={fmt_bytes(p.memory_info().rss)}\")\n",
    "            parts.append(f\"cpu%~{p.cpu_percent(interval=0.0):.1f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if _tracemalloc and _tracemalloc.is_tracing():\n",
    "        try:\n",
    "            cur, peak = _tracemalloc.get_traced_memory()\n",
    "            parts.append(f\"py_mem={fmt_bytes(cur)}/{fmt_bytes(peak)}(peak)\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if parts:\n",
    "        logger.debug(\"RES%s %s\", f\"[{note}]\" if note else \"\", \" \".join(parts))\n",
    "\n",
    "# --- Project defaults ---\n",
    "DEST_ROOT = \"../data\"\n",
    "\n",
    "def resolve_path(rel: str | None, filename: str) -> Path:\n",
    "    return Path(rel) / filename if rel else Path(DEST_ROOT) / filename\n",
    "\n",
    "def load_project_files(\n",
    "    file_map: dict[str, str],\n",
    "    project_root_name: str = \"ds_code_challenge\",\n",
    "    inject_globals: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load files from ROOT/data based on a {file_name: variable_name} map.\n",
    "\n",
    "    Supports:\n",
    "      - .csv / .csv.gz  -> pandas DataFrame\n",
    "      - .geojson / .geojson.gz -> GeoDataFrame\n",
    "      - .ods -> Excel (odf engine)\n",
    "    \"\"\"\n",
    "    with timed(\"load_project_files (resolve ROOT/DATA_DIR)\"):\n",
    "        ROOT = Path(__file__).resolve().parents[0] if \"__file__\" in globals() else Path().resolve()\n",
    "        while ROOT.name != project_root_name and ROOT.parent != ROOT:\n",
    "            ROOT = ROOT.parent\n",
    "        DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "    results: dict[str, pd.DataFrame | gpd.GeoDataFrame] = {}\n",
    "    resource_snapshot(\"load_project_files:start\")\n",
    "\n",
    "    for file_name, var_name in file_map.items():\n",
    "        file_path = DATA_DIR / file_name\n",
    "        logger.info(\"Processing %s...\", file_path)\n",
    "\n",
    "        with timed(f\"read:{file_path.name}\"):\n",
    "            suffix = \"\".join(file_path.suffixes).lower()\n",
    "\n",
    "            if suffix.endswith((\".csv\", \".csv.gz\")):\n",
    "                df = pd.read_csv(file_path)\n",
    "                results[var_name] = df\n",
    "                logger.info(\"→ %s loaded as DataFrame shape=%s\", var_name, getattr(df, \"shape\", None))\n",
    "\n",
    "            elif suffix.endswith((\".geojson\", \".geojson.gz\")):\n",
    "                gdf = gpd.read_file(file_path)\n",
    "                results[var_name] = gdf\n",
    "                logger.info(\"→ %s loaded as GeoDataFrame len=%d\", var_name, len(gdf))\n",
    "\n",
    "            elif suffix.endswith(\".ods\"):\n",
    "                df = pd.read_excel(file_path, engine=\"odf\")\n",
    "                results[var_name] = df\n",
    "                logger.info(\"→ %s loaded from ODS shape=%s\", var_name, getattr(df, \"shape\", None))\n",
    "\n",
    "            else:\n",
    "                logger.warning(\"Skipping unsupported file type: %s\", file_path)\n",
    "                continue\n",
    "\n",
    "            if inject_globals:\n",
    "                globals()[var_name] = results[var_name]\n",
    "\n",
    "            resource_snapshot(f\"after_load:{file_path.name}\")\n",
    "\n",
    "    logger.info(\"All files loaded successfully.\")\n",
    "    resource_snapshot(\"load_project_files:end\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a67c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_csv(df: pd.DataFrame, file_name: str, index: bool = False):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a CSV file in the project's data directory.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        file_name (str): The name of the output file (e.g., 'anonymized_data.csv').\n",
    "        index (bool, optional): Whether to write row names (index). Defaults to False.\n",
    "    \"\"\"\n",
    "    # Define the root directory of your project\n",
    "    ROOT = Path.cwd().parent\n",
    "    \n",
    "    # Define the data directory and ensure it exists\n",
    "    DATA_DIR = ROOT / \"output\"\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Ensure the filename ends with .csv\n",
    "    if not file_name.endswith('.csv'):\n",
    "        file_name += '.csv'\n",
    "        \n",
    "    file_path = DATA_DIR / file_name\n",
    "    \n",
    "    # Save the DataFrame to CSV with configurable index\n",
    "    df.to_csv(file_path, index=index)\n",
    "    \n",
    "    print(f\"DataFrame successfully saved to: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c14e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_file(url: str, filename: str = \"Wind_direction_and_speed_2020.ods\", rel: str | None = None):\n",
    "    \"\"\"Download a file into project data dir.\"\"\"\n",
    "    save_path = resolve_path(rel, filename)\n",
    "    ensure_dir(save_path.parent)\n",
    "\n",
    "    with timed(f\"download:{filename}\"):\n",
    "        with requests.get(url, stream=True, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            bytes_written = 0\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        bytes_written += len(chunk)\n",
    "\n",
    "        logger.info(\"File downloaded to %s (%s)\", save_path.resolve(), fmt_bytes(bytes_written))\n",
    "        resource_snapshot(f\"after_download:{filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6e356",
   "metadata": {},
   "source": [
    "# Downloads wind and speed direction ods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf969cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.capetown.gov.za/_layouts/OpenDataPortalHandler/DownloadHandler.ashx?DocumentName=Wind_direction_and_speed_2020.ods&DatasetDocument=https%3A%2F%2Fcityapps.capetown.gov.za%2Fsites%2Fopendatacatalog%2FDocuments%2FWind%2FWind_direction_and_speed_2020.ods\"\n",
    "download_file(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_centroid_latlon(\n",
    "    suburb_name: str,\n",
    "    *,\n",
    "    featureserver_url: str,\n",
    "    epsg_utm: int | None = 32734,\n",
    "    req_timeout: int = 60,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Return (lat, lon) centroid for official suburb polygon.\"\"\"\n",
    "    params = {\n",
    "        \"where\": f\"OFC_SBRB_NAME='{suburb_name}'\",\n",
    "        \"outFields\": \"*\",\n",
    "        \"returnGeometry\": \"true\",\n",
    "        \"outSR\": \"4326\",\n",
    "        \"f\": \"geojson\",\n",
    "    }\n",
    "\n",
    "    with timed(f\"fetch_centroid_latlon[{suburb_name}]\"):\n",
    "        r = requests.get(featureserver_url, params=params, timeout=req_timeout)\n",
    "        r.raise_for_status()\n",
    "        gj = r.json()\n",
    "        resource_snapshot(\"after_download\")\n",
    "\n",
    "        feats = gj.get(\"features\", [])\n",
    "        if not feats:\n",
    "            raise RuntimeError(f\"No polygon returned for suburb='{suburb_name}'\")\n",
    "\n",
    "        gdf = gpd.GeoDataFrame.from_features(feats, crs=\"EPSG:4326\")\n",
    "        if len(gdf) > 1:\n",
    "            gdf[\"ones\"] = 1\n",
    "            gdf = gdf.dissolve(by=\"ones\", as_index=False, aggfunc=\"first\").drop(columns=\"ones\")\n",
    "\n",
    "        # pick a projected CRS for accurate centroid if needed\n",
    "        def _auto_utm_epsg(gdf_ll: gpd.GeoDataFrame) -> int:\n",
    "            c_ll = gdf_ll.unary_union.centroid\n",
    "            lon0, lat0 = float(c_ll.x), float(c_ll.y)\n",
    "            zone = int((lon0 + 180) // 6 + 1)\n",
    "            return (32600 + zone) if lat0 >= 0 else (32700 + zone)\n",
    "\n",
    "        epsg_proj = (epsg_utm if epsg_utm not in (None, 4326) else _auto_utm_epsg(gdf))\n",
    "        if epsg_utm in (None, 4326):\n",
    "            logger.info(\"Using projected CRS EPSG:%d for centroid\", epsg_proj)\n",
    "\n",
    "        # project polygon → centroid in meters\n",
    "        gdf_proj = gdf.to_crs(epsg=epsg_proj)\n",
    "        c_proj = gdf_proj.geometry.centroid.iloc[0]  # projected centroid (meters)\n",
    "\n",
    "        # transform that single point back to WGS84 as true scalars\n",
    "        transformer = Transformer.from_crs(epsg_proj, 4326, always_xy=True)\n",
    "        cent_lon, cent_lat = transformer.transform(float(c_proj.x), float(c_proj.y))\n",
    "\n",
    "        logger.info(\"Centroid = (%.6f, %.6f)\", cent_lat, cent_lon)\n",
    "        resource_snapshot(\"after_centroid\")\n",
    "\n",
    "    return float(cent_lat), float(cent_lon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_map = {\n",
    "    \"Wind_direction_and_speed_2020.ods\": \"df_wind\",\n",
    "    \"sr_hex.csv\": \"df_sr_hex\",\n",
    "}\n",
    "\n",
    "results = load_project_files(file_map)\n",
    "\n",
    "# Access DataFrames from results (and from globals if inject_globals=True)\n",
    "df_wind   = results[\"df_wind\"]\n",
    "df_sr_hex = results[\"df_sr_hex\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Vectorized great-circle distance in km.\"\"\"\n",
    "    R = 6371.0\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi    = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2.0)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def filter_within_radius_km(\n",
    "\n",
    "    df_latlon: pd.DataFrame,\n",
    "    cent_lat: float,\n",
    "    cent_lon: float,\n",
    "    *,\n",
    "    radius_km: float = 1.852,\n",
    "    lat_col: str = \"latitude\",\n",
    "    lon_col: str = \"longitude\",\n",
    "    keep_distance_col: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters out service requests beyond a certain radius from the centroid\n",
    "    \"\"\"\n",
    "    with timed(f\"filter_within_radius_km[{radius_km:.3f} km]\"):\n",
    "        mask_valid = df_latlon[lat_col].notna() & df_latlon[lon_col].notna()\n",
    "        n_invalid = (~mask_valid).sum()\n",
    "        if n_invalid:\n",
    "            logger.info(\"Skipped %d rows with missing coordinates\", int(n_invalid))\n",
    "\n",
    "        df = df_latlon.loc[mask_valid].copy()\n",
    "        df[\"distance_km\"] = haversine_km(\n",
    "            df[lat_col].astype(float).values,\n",
    "            df[lon_col].astype(float).values,\n",
    "            cent_lat, cent_lon\n",
    "        )\n",
    "        out = df.loc[df[\"distance_km\"] <= radius_km].copy()\n",
    "        if not keep_distance_col:\n",
    "            out.drop(columns=[\"distance_km\"], inplace=True)\n",
    "\n",
    "        logger.info(\"Kept %d/%d valid rows within %.3f km\",\n",
    "                    len(out), len(df), radius_km)\n",
    "        resource_snapshot(\"after_radius_filter\")\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78206d3",
   "metadata": {},
   "source": [
    "# Created subsample of sr_hex.csv and get centroid\n",
    "This was done by using the suburb’s official name to pull its polygon from the City of Cape Town’s Map Viewer. From there, I used geometry.centroid to extract the centroid latitude and longitude. I then defined a circle around that point based on 1 minute of latitude (≈ 1.852 km). Latitude was chosen since it’s a consistent angular measure globally, whereas longitude varies by latitude. Finally, I calculated the vectorized haversine distance between the centroid and each service request in the CSV, and filtered down to those falling within 1.852 km.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURESERVER_URL = \"https://citymaps.capetown.gov.za/agsext/rest/services/Theme_Based/ODP_SPLIT_5/FeatureServer/3/query\"\n",
    "cent_lat, cent_lon = fetch_centroid_latlon(\"BELLVILLE SOUTH\", featureserver_url=FEATURESERVER_URL)\n",
    "df_1min_bellville = filter_within_radius_km(df_sr_hex, cent_lat, cent_lon)\n",
    "save_df_to_csv(df_1min_bellville, \"1min_bellville.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b052a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def _infer_site_wind_cols(df: pd.DataFrame, site_name: str) -> tuple[str, str]:\n",
    "    cols = df.columns.astype(str)\n",
    "    site_re = re.escape(site_name)\n",
    "    dir_re = re.compile(rf\"(?i){site_re}.*_Deg$\")\n",
    "    spd_re = re.compile(rf\"(?i){site_re}.*_(m/s|m_s|ms)$\")\n",
    "\n",
    "    dir_candidates = [c for c in cols if dir_re.search(c)]\n",
    "    spd_candidates = [c for c in cols if spd_re.search(c)]\n",
    "\n",
    "    if not dir_candidates:\n",
    "        dir_candidates = [c for c in cols if re.search(rf\"(?i){site_re}.*deg\", c)]\n",
    "    if not spd_candidates:\n",
    "        spd_candidates = [c for c in cols if re.search(rf\"(?i){site_re}.*(m/s|m_s|ms|speed)\", c)]\n",
    "\n",
    "    if not dir_candidates or not spd_candidates:\n",
    "        raise ValueError(\n",
    "            f\"Could not infer wind columns for site '{site_name}'. \"\n",
    "            f\"Found direction candidates: {dir_candidates or '[]'}, \"\n",
    "            f\"speed candidates: {spd_candidates or '[]'}\"\n",
    "        )\n",
    "\n",
    "    dir_col = sorted(dir_candidates, key=len)[0]\n",
    "    spd_col = sorted(spd_candidates, key=len)[0]\n",
    "    logging.info(\"Inferred columns for '%s' -> dir: %s | spd: %s\", site_name, dir_col, spd_col)\n",
    "    return dir_col, spd_col\n",
    "\n",
    "def process_wind_data(file_path: str, site_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process wind data for a given site.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the Excel/ODS file.\n",
    "    site_name : str\n",
    "        Name (or part of it) of the site to filter columns on (case-insensitive).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Processed DataFrame with DateTime index and site-specific columns.\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    logging.info(\"Processing: %s\", file_path)\n",
    "\n",
    "    # 1) Read file with 2-row header\n",
    "    df = pd.read_excel(file_path, engine=\"odf\", header=[1, 2], skiprows=[0, 3])\n",
    "\n",
    "    # 2) Flatten header\n",
    "    df.columns = pd.Index([\n",
    "        (str(a).strip() if (pd.isna(b) or \"Unnamed\" in str(b))\n",
    "         else f\"{str(a).strip()}_{str(b).strip()}\")\n",
    "        for a, b in df.columns\n",
    "    ]).str.replace(\" \", \"_\", regex=False)\n",
    "\n",
    "    # 3) Keep first DateTime column only\n",
    "    dt_col = next(c for c in df.columns if \"date\" in c.lower() and \"time\" in c.lower())\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[dt_col], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "    # 4) Keep DateTime + site-specific cols only, force numeric\n",
    "    logger.debug(df.head())\n",
    "    keep_mask = df.columns.str.contains(site_name, case=False) | (df.columns == \"DateTime\")\n",
    "    df = df.loc[:, keep_mask]\n",
    "    site_cols = df.columns[df.columns.str.contains(site_name, case=False)]\n",
    "    df[site_cols] = df[site_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    # 5) Drop rows where DateTime is NaT\n",
    "    n_before = len(df)\n",
    "    df = df.dropna(subset=[\"DateTime\"])\n",
    "    n_dropped = n_before - len(df)\n",
    "    if n_dropped:\n",
    "        logging.info(\"Dropped %d rows with NaT DateTime\", n_dropped)\n",
    "\n",
    "    # 6) Deduplicate and set DateTime index\n",
    "    df = df.drop_duplicates(subset=[\"DateTime\"]).set_index(\"DateTime\").sort_index()\n",
    "\n",
    "    logging.info(\"Done. Shape: %s. Elapsed: %.3fs\", df.shape, time.perf_counter() - t0)\n",
    "    return df\n",
    "\n",
    "# --- New: rename to generic labels ---\n",
    "def standardize_wind_column_names(\n",
    "    df: pd.DataFrame,\n",
    "    site_name: str,\n",
    "    dir_label: str = \"wind direction degree\",\n",
    "    spd_label: str = \"wind speed m/s\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Locate a site's wind direction/speed columns and rename them to generic labels.\n",
    "    Returns a copy with columns renamed.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    dir_col, spd_col = _infer_site_wind_cols(df2, site_name)\n",
    "\n",
    "    # Build a safe, minimal mapping\n",
    "    rename_map = {dir_col: dir_label, spd_col: spd_label}\n",
    "    df2 = df2.rename(columns=rename_map)\n",
    "\n",
    "    logging.info(\"Renamed '%s' -> '%s', '%s' -> '%s'\", dir_col, dir_label, spd_col, spd_label)\n",
    "    return df2\n",
    "\n",
    "# --- Updated: fill uses standardized labels by default ---\n",
    "def fill_wind_missing(df: pd.DataFrame,\n",
    "                      dir_col: str = \"wind direction degree\",\n",
    "                      spd_col: str = \"wind speed m/s\") -> pd.DataFrame:\n",
    "    \"\"\"Fills in missing wind speed and direction data with averages\"\"\"\n",
    "    with timed(\"fill_wind_missing\"):\n",
    "        if not np.issubdtype(df.index.dtype, np.datetime64):\n",
    "            raise ValueError(\"Index must be datetime for interpolation.\")\n",
    "\n",
    "        before = df[[dir_col, spd_col]].isna().sum()\n",
    "        logger.info(\"NaNs before | %s=%d, %s=%d\",\n",
    "                    dir_col, int(before[dir_col]),\n",
    "                    spd_col, int(before[spd_col]))\n",
    "\n",
    "        # Speed interpolation\n",
    "        df[spd_col] = pd.to_numeric(df[spd_col], errors=\"coerce\").interpolate(\n",
    "            method=\"time\", limit_direction=\"both\"\n",
    "        )\n",
    "\n",
    "        # Direction circular interpolation\n",
    "        theta = np.deg2rad(pd.to_numeric(df[dir_col], errors=\"coerce\"))\n",
    "        sin_i = pd.Series(np.sin(theta), index=df.index).interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        cos_i = pd.Series(np.cos(theta), index=df.index).interpolate(method=\"time\", limit_direction=\"both\")\n",
    "        r = np.hypot(sin_i, cos_i)\n",
    "        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "            df[dir_col] = (np.degrees(np.arctan2(sin_i / r, cos_i / r)) % 360.0)\n",
    "\n",
    "        after = df[[dir_col, spd_col]].isna().sum()\n",
    "        logger.info(\"NaNs after  | %s=%d, %s=%d\",\n",
    "                    dir_col, int(after[dir_col]),\n",
    "                    spd_col, int(after[spd_col]))\n",
    "        resource_snapshot(\"after_fill\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbba231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Helpers\n",
    "# --------------------------------------------------------------------\n",
    "def prep_sr_timestamps(\n",
    "    df_sr: pd.DataFrame,\n",
    "    *,\n",
    "    ts_col: str = \"creation_timestamp\",\n",
    "    out_round_col: str = \"creation_rounded\",\n",
    "    cutoff: pd.Timestamp = pd.Timestamp(\"2020-12-31 23:00\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare SR timestamps:\n",
    "      - parse ts, drop invalid\n",
    "      - make tz-naive\n",
    "      - out_round_col rule:\n",
    "          if ts < cutoff  -> round to 30min\n",
    "          if ts >= cutoff -> set exactly to cutoff\n",
    "      - return filtered copy with valid out_round_col\n",
    "    \"\"\"\n",
    "    sr = df_sr.copy()\n",
    "    sr[ts_col] = pd.to_datetime(sr[ts_col], errors=\"coerce\")\n",
    "    n_bad = sr[ts_col].isna().sum()\n",
    "    if n_bad:\n",
    "        logger.info(\"Dropped %d SR rows with invalid %s\", n_bad, ts_col)\n",
    "\n",
    "    # Make tz-naive BEFORE comparisons/rounding\n",
    "    try:\n",
    "        sr[ts_col] = sr[ts_col].dt.tz_localize(None)\n",
    "    except (TypeError, AttributeError):\n",
    "        pass\n",
    "\n",
    "    # Apply rounding/cutoff rule\n",
    "    sr[out_round_col] = sr[ts_col].where(\n",
    "        sr[ts_col] >= cutoff,\n",
    "        sr[ts_col].dt.round(\"30min\")\n",
    "    ).mask(sr[ts_col] >= cutoff, cutoff)\n",
    "\n",
    "    # Make rounded col tz-naive (defensive)\n",
    "    try:\n",
    "        sr[out_round_col] = sr[out_round_col].dt.tz_localize(None)\n",
    "    except (TypeError, AttributeError):\n",
    "        pass\n",
    "\n",
    "    # Keep only rows with a valid rounded timestamp\n",
    "    sr = sr.loc[sr[out_round_col].notna()].copy()\n",
    "\n",
    "    n_forced = (sr[ts_col] >= cutoff).sum()\n",
    "    if n_forced:\n",
    "        logger.info(\"Forced %d SR timestamps to cutoff %s\", n_forced, cutoff)\n",
    "\n",
    "    logger.info(\"SR rows retained: %d\", len(sr))\n",
    "    return sr\n",
    "\n",
    "\n",
    "def build_half_hour_grid_with_circular_dir(aqm: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a 30‑minute grid from hourly AQM readings:\n",
    "      - :00 (original) kept as-is\n",
    "      - :30 is arithmetic mean of t and t+1 for all numeric columns,\n",
    "        EXCEPT wind direction uses circular interpolation.\n",
    "      - Wind speed remains a simple arithmetic average (old behavior).\n",
    "    Column names assumed:\n",
    "      - \"wind direction degree\"   (degrees 0..360)\n",
    "      - \"wind speed m/s\"          (m/s)\n",
    "    \"\"\"\n",
    "    aqm_00 = aqm\n",
    "\n",
    "    # Start with arithmetic half-hour means for all numeric columns\n",
    "    aqm_30 = (aqm + aqm.shift(-1)) / 2\n",
    "\n",
    "    # Circular interpolation for wind direction ONLY (no speed-weighting)\n",
    "    if \"wind direction degree\" in aqm.columns:\n",
    "        d0 = np.deg2rad(aqm[\"wind direction degree\"])\n",
    "        d1 = np.deg2rad(aqm[\"wind direction degree\"].shift(-1))\n",
    "\n",
    "        valid = d0.notna() & d1.notna()\n",
    "\n",
    "        u0, v0 = np.cos(d0), np.sin(d0)\n",
    "        u1, v1 = np.cos(d1), np.sin(d1)\n",
    "        u_half = (u0 + u1) / 2.0\n",
    "        v_half = (v0 + v1) / 2.0\n",
    "\n",
    "        dir_half = (np.degrees(np.arctan2(v_half, u_half)) + 360.0) % 360.0\n",
    "        dir_half = dir_half.where(valid)  # keep NaN if either endpoint is NaN\n",
    "\n",
    "        # Overwrite arithmetic mean for direction with circular result\n",
    "        aqm_30[\"wind direction degree\"] = dir_half\n",
    "\n",
    "    # Index for half-hour\n",
    "    aqm_30.index = aqm_30.index + pd.Timedelta(minutes=30)\n",
    "\n",
    "    # Combine :00 and :30\n",
    "    aqm_30min = pd.concat([aqm_00, aqm_30]).sort_index()\n",
    "    logger.info(\"AQM 30‑min grid rows: %d\", len(aqm_30min))\n",
    "    return aqm_30min\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Orchestrator\n",
    "# --------------------------------------------------------------------\n",
    "def join_sr_to_aqm_fast(\n",
    "    df_sr: pd.DataFrame,\n",
    "    df_aqm: pd.DataFrame,\n",
    "    ts_col: str = \"creation_timestamp\",\n",
    "    out_round_col: str = \"creation_rounded\",\n",
    "    cutoff: pd.Timestamp = pd.Timestamp(\"2020-12-31 23:00\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorized join of SR to AQM:\n",
    "      - SR timestamps parsed & rounded via `prep_sr_timestamps`\n",
    "      - AQM hourly → 30‑min grid via `build_half_hour_grid_with_circular_dir`\n",
    "      - Coerce AQM to numeric (non-numeric → NaN)\n",
    "      - Merge on 30‑min timestamps\n",
    "    \"\"\"\n",
    "    with timed(\"join_sr_to_aqm_fast:TOTAL\"):\n",
    "        # ---------- SR prep ----------\n",
    "        with timed(\"join_sr_to_aqm_fast:SR_preparation\"):\n",
    "            sr = prep_sr_timestamps(\n",
    "                df_sr, ts_col=ts_col, out_round_col=out_round_col, cutoff=cutoff\n",
    "            )\n",
    "            resource_snapshot(\"join_sr_to_aqm_fast:after_SR_prep\")\n",
    "\n",
    "        # ---------- AQM prep ----------\n",
    "        with timed(\"join_sr_to_aqm_fast:AQM_preparation\"):\n",
    "            aqm = df_aqm.copy()\n",
    "            aqm.index = pd.to_datetime(aqm.index, errors=\"coerce\")\n",
    "            aqm = aqm.loc[aqm.index.notna()].sort_index()\n",
    "\n",
    "            try:\n",
    "                aqm.index = aqm.index.tz_localize(None)\n",
    "            except (TypeError, AttributeError):\n",
    "                pass\n",
    "\n",
    "            if aqm.index.has_duplicates:\n",
    "                dup_cnt = aqm.index.duplicated().sum()\n",
    "                logger.info(\"Found %d duplicate AQM timestamps; aggregating with mean\", dup_cnt)\n",
    "                aqm = aqm.groupby(aqm.index).mean(numeric_only=True)\n",
    "\n",
    "            logger.info(\"AQM rows after datetime prep: %d\", len(aqm))\n",
    "            resource_snapshot(\"join_sr_to_aqm_fast:after_AQM_prep\")\n",
    "\n",
    "        # ---------- Numeric coercion ----------\n",
    "        with timed(\"join_sr_to_aqm_fast:AQM_numeric_coercion\"):\n",
    "            non_numeric_before = (~aqm.stack().map(np.isreal)).sum()\n",
    "            aqm = aqm.apply(pd.to_numeric, errors=\"coerce\")\n",
    "            total_nans_after = aqm.isna().sum().sum()\n",
    "            logger.info(\n",
    "                \"AQM non‑numeric→NaN approx: %d; total NaNs after coercion: %d\",\n",
    "                non_numeric_before, total_nans_after\n",
    "            )\n",
    "            if total_nans_after > 0:\n",
    "                nan_rows = aqm[aqm.isna().any(axis=1)].head(5)\n",
    "                logger.info(\"Sample NaN rows (up to 5):\\n%s\", nan_rows)\n",
    "            resource_snapshot(\"join_sr_to_aqm_fast:after_numeric\")\n",
    "\n",
    "        # ---------- Build 30‑minute grid (circular dir, speed arithmetic) ----------\n",
    "        with timed(\"join_sr_to_aqm_fast:build_30min_grid\"):\n",
    "            aqm_30min = build_half_hour_grid_with_circular_dir(aqm)\n",
    "            resource_snapshot(\"join_sr_to_aqm_fast:after_grid\")\n",
    "\n",
    "        # ---------- Join ----------\n",
    "        with timed(\"join_sr_to_aqm_fast:merge\"):\n",
    "            out = sr.merge(aqm_30min, left_on=out_round_col, right_index=True, how=\"left\")\n",
    "            logger.info(\"Join output shape: %s\", out.shape)\n",
    "\n",
    "            # quick visibility on unmatched rows (all-NaN AQM fields)\n",
    "            unmatched = out[out.isna().all(axis=1)].shape[0]\n",
    "            if unmatched:\n",
    "                logger.info(\"Rows with all NaNs after join (possible no AQM match): %d\", unmatched)\n",
    "\n",
    "            resource_snapshot(\"join_sr_to_aqm_fast:after_join\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bd037",
   "metadata": {},
   "source": [
    "# This augments filtered Bellville South service requests with wind data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a359f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bv_wind_processed = process_wind_data(\"../data/Wind_direction_and_speed_2020.ods\", site_name=\"Bellville\")\n",
    "df_std = standardize_wind_column_names(df_bv_wind_processed, site_name=\"Bellville\")\n",
    "save_df_to_csv(df_std, \"bv_wind_processed.csv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f326ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filled = fill_wind_missing(df_std)\n",
    "save_df_to_csv(df_filled, \"bv_wind_filled.csv\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278face",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_with_wind = join_sr_to_aqm_fast(df_sr=df_1min_bellville, df_aqm=df_filled)\n",
    "sr_with_wind[\"completion_timestamp\"] = pd.to_datetime(sr_with_wind[\"completion_timestamp\"], errors=\"coerce\")\n",
    "sr_with_wind[\"completion_timestamp\"] = sr_with_wind[\"completion_timestamp\"].dt.tz_localize(None)\n",
    "save_df_to_csv(sr_with_wind, \"sr_with_wind.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08c441c",
   "metadata": {},
   "source": [
    "# Anonymize augmented subsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de30a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def anonymize_sr_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Anonymizes a service‑request DataFrame by:\n",
    "      - Adding Entry_number = 1..N (surrogate ID).\n",
    "      - Dropping direct identifiers and precise coordinates/time‑derived fields.\n",
    "      - Generalizing timestamps to 6‑hour blocks (tz‑naive).\n",
    "      - Keeping spatial context via H3 index only (no raw lat/lon).\n",
    "\n",
    "    Preserves existing timers/logging/resource snapshots.\n",
    "    \"\"\"\n",
    "    columns_to_drop = [\n",
    "        \"notification_number\",\n",
    "        \"reference_number\",\n",
    "        \"distance_km\",\n",
    "        \"creation_rounded\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"code\",\n",
    "        \"cause_code\",\n",
    "    ]\n",
    "\n",
    "    with timed(\"anonymize_sr_data:TOTAL\"):\n",
    "        with timed(\"anonymize_sr_data:drop_columns\"):\n",
    "            df_anonymized = df.copy()\n",
    "\n",
    "            # Add sequential surrogate ID (1..N) before dropping anything\n",
    "            df_anonymized.insert(0, \"Entry_number\", np.arange(1, len(df_anonymized) + 1, dtype=np.int64))\n",
    "\n",
    "            present = [c for c in columns_to_drop if c in df_anonymized.columns]\n",
    "            missing = [c for c in columns_to_drop if c not in df_anonymized.columns]\n",
    "            if missing:\n",
    "                logger.debug(\"Columns not present (ignored): %s\", missing)\n",
    "\n",
    "            df_anonymized = df_anonymized.drop(columns=present, errors=\"ignore\")\n",
    "            logger.info(\"Dropped %d columns; resulting cols=%d\", len(present), df_anonymized.shape[1])\n",
    "            resource_snapshot(\"anonymize_sr_data:after_drop\")\n",
    "\n",
    "        with timed(\"anonymize_sr_data:timestamp_generalization\"):\n",
    "            # Generalize to 6‑hour blocks, keep tz‑naive for consistent joins/storage\n",
    "            for c in (\"creation_timestamp\", \"completion_timestamp\"):\n",
    "                if c in df_anonymized.columns:\n",
    "                    df_anonymized[c] = pd.to_datetime(df_anonymized[c], errors=\"coerce\")\n",
    "                    try:\n",
    "                        df_anonymized[c] = df_anonymized[c].dt.tz_localize(None)\n",
    "                    except (TypeError, AttributeError):\n",
    "                        # Already tz‑naive or column missing dtype support\n",
    "                        pass\n",
    "                    df_anonymized[c] = df_anonymized[c].dt.floor(\"6h\")\n",
    "\n",
    "            # quick stat on rows affected\n",
    "            n_na_create = df_anonymized[\"creation_timestamp\"].isna().sum() if \"creation_timestamp\" in df_anonymized else 0\n",
    "            n_na_complete = df_anonymized[\"completion_timestamp\"].isna().sum() if \"completion_timestamp\" in df_anonymized else 0\n",
    "            logger.info(\"Timestamp NaNs | creation=%d, completion=%d\", n_na_create, n_na_complete)\n",
    "            resource_snapshot(\"anonymize_sr_data:after_time_gen\")\n",
    "\n",
    "        logger.info(\"Anonymized SR shape: %s\", df_anonymized.shape)\n",
    "\n",
    "    return df_anonymized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_anonymized = anonymize_sr_data(sr_with_wind)\n",
    "save_df_to_csv(sr_anonymized, \"anon_sr_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dcf4d8",
   "metadata": {},
   "source": [
    "The function above anonymizes a service request DataFrame by removing identifying columns and reducing temporal precision. Spatial data is generalized by keeping only H3 index. H3 index was chosen at resolution 8 since it features an average length of 461 meters and farthest distance from the centroid is 533 meters, this level of granularity is fine for the purpose of this exercise then.\n",
    "\n",
    "Columns dropped were notification number, reference number, distance km, creation rounded, latitude, longitude, code, and cause code. Distance km is something that I created for calcualting the 1 minute from the centroid of Bellville. Code and cause were dropped as they seem to be pretty detailed and can make individual complaints unique and reduces identification risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d2c6b",
   "metadata": {},
   "source": [
    "# Unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92954941",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -q\n",
    "\n",
    "# ---- Utilities ---------------------------------------------------------------\n",
    "\n",
    "def _make_tmp_tree(tmp_path: Path, name=\"ds_code_challenge\"):\n",
    "    \"\"\"Create a fake project root with a /data dir and return (root, data_dir).\"\"\"\n",
    "    root = tmp_path / name\n",
    "    data = root / \"data\"\n",
    "    data.mkdir(parents=True, exist_ok=True)\n",
    "    return root, data\n",
    "\n",
    "# ---- Basic helpers: ensure_dir / parent_dir_of / fmt_bytes -------------------\n",
    "\n",
    "def test_ensure_dir_creates_directory(tmp_path):\n",
    "    target = tmp_path / \"a\" / \"b\" / \"c\"\n",
    "    assert not target.exists()\n",
    "    nb.ensure_dir(target)\n",
    "    assert target.exists() and target.is_dir()\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"inp,expect\",\n",
    "    [\n",
    "        (\"/a/b/c.txt\", \"/a/b\"),\n",
    "        (\"/a/b/\", \"/a/b\"),\n",
    "        (\"justafile\", \".\"),\n",
    "        (\"\", \".\"),\n",
    "    ],\n",
    ")\n",
    "def test_parent_dir_of(inp, expect):\n",
    "    got = nb.parent_dir_of(inp)\n",
    "    assert got == expect\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"n,expect\",\n",
    "    [\n",
    "        (0, \"0.0B\"),\n",
    "        (10, \"10.0B\"),\n",
    "        (1023, \"1023.0B\"),\n",
    "        (1024, \"1.0KB\"),\n",
    "        (1024**2 * 1.5, \"1.5MB\"),\n",
    "        (\"bad\", \"bad\"),  # passthrough on exception\n",
    "    ],\n",
    ")\n",
    "def test_fmt_bytes(n, expect):\n",
    "    assert nb.fmt_bytes(n) == expect\n",
    "\n",
    "# ---- gunzip_file -------------------------------------------------------------\n",
    "\n",
    "def test_gunzip_file_roundtrip(tmp_path):\n",
    "    # Create gz source\n",
    "    src = tmp_path / \"x.csv.gz\"\n",
    "    dst = tmp_path / \"out.csv\"\n",
    "    raw = b\"col1,col2\\n1,2\\n3,4\\n\"\n",
    "    with gzip.open(src, \"wb\") as f:\n",
    "        f.write(raw)\n",
    "\n",
    "    nb.gunzip_file(str(src), str(dst))\n",
    "    assert dst.exists()\n",
    "    assert dst.read_bytes() == raw\n",
    "\n",
    "# ---- client_error_msg --------------------------------------------------------\n",
    "\n",
    "class _FakeBotoError(Exception):\n",
    "    def __init__(self, code=None, message=None):\n",
    "        self.response = {\"Error\": {}}\n",
    "        if code is not None:\n",
    "            self.response[\"Error\"][\"Code\"] = code\n",
    "        if message is not None:\n",
    "            self.response[\"Error\"][\"Message\"] = message\n",
    "        super().__init__(message or \"\")\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"code,msg,expect_regex\",\n",
    "    [\n",
    "        (\"AccessDenied\", \"Nope\", r\"\\[AccessDenied\\] Nope\"),\n",
    "        (None, \"OnlyMsg\", r\"OnlyMsg\"),\n",
    "        (None, None, r\"^$\"),  # falls back to str(e) which is empty here\n",
    "    ],\n",
    ")\n",
    "def test_client_error_msg_formats(code, msg, expect_regex):\n",
    "    e = _FakeBotoError(code, msg)\n",
    "    out = nb.client_error_msg(e)\n",
    "    assert re.search(expect_regex, out) is not None\n",
    "\n",
    "# ---- timed() & resource_snapshot() ------------------------------------------\n",
    "\n",
    "def test_timed_logs_elapsed(caplog):\n",
    "    caplog.set_level(\"INFO\")\n",
    "    with nb.timed(\"unit-test\"):\n",
    "        time.sleep(0.01)\n",
    "    # Look for \"... took X.XXX s\"\n",
    "    assert any(\"unit-test took \" in rec.getMessage() for rec in caplog.records)\n",
    "\n",
    "def test_resource_snapshot_safe_when_not_debug(caplog):\n",
    "    caplog.set_level(\"INFO\")\n",
    "    nb.resource_snapshot(\"noop\")  # should not raise, may log nothing\n",
    "    assert True  # reached\n",
    "\n",
    "# ---- resolve_path ------------------------------------------------------------\n",
    "\n",
    "def test_resolve_path_with_rel_and_default(tmp_path, monkeypatch):\n",
    "    # Default DEST_ROOT path\n",
    "    monkeypatch.setattr(nb, \"DEST_ROOT\", str(tmp_path / \"data\"))\n",
    "    p1 = nb.resolve_path(None, \"file.csv\")\n",
    "    assert p1 == tmp_path / \"data\" / \"file.csv\"\n",
    "\n",
    "    p2 = nb.resolve_path(str(tmp_path / \"alt\"), \"file.csv\")\n",
    "    assert p2 == tmp_path / \"alt\" / \"file.csv\"\n",
    "\n",
    "# ---- save_df_to_csv (first definition with rel arg) --------------------------\n",
    "# If your notebook overwrote save_df_to_csv later, this test is skipped.\n",
    "save_has_rel = \"rel\" in nb.save_df_to_csv.__code__.co_varnames\n",
    "\n",
    "@pytest.mark.skipif(not save_has_rel, reason=\"save_df_to_csv(rel=...) not present (overwritten later).\")\n",
    "def test_save_df_to_csv_with_rel(tmp_path, caplog, monkeypatch):\n",
    "    caplog.set_level(\"INFO\")\n",
    "    monkeypatch.setattr(nb, \"DEST_ROOT\", str(tmp_path / \"data\"))\n",
    "    df = pd.DataFrame({\"a\": [1, 2]})\n",
    "    nb.save_df_to_csv(df, \"test.csv\", rel=str(tmp_path / \"out\"))\n",
    "    out = tmp_path / \"out\" / \"test.csv\"\n",
    "    assert out.exists()\n",
    "    got = pd.read_csv(out)\n",
    "    pd.testing.assert_frame_equal(got, df)\n",
    "\n",
    "# ---- save_df_to_csv (second definition without rel arg) ----------------------\n",
    "\n",
    "@pytest.mark.skipif(save_has_rel, reason=\"Testing the non-rel version only when it overwrote the first.\")\n",
    "def test_save_df_to_csv_no_rel(tmp_path, monkeypatch):\n",
    "    # Fake cwd so function saves under <cwd>/.. / output\n",
    "    class _FakePath(Path):\n",
    "        _flavour = Path(\".\")._flavour\n",
    "    def _fake_cwd():\n",
    "        return _FakePath(str(tmp_path / \"proj\" / \"work\"))\n",
    "    (tmp_path / \"proj\" / \"work\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    monkeypatch.setattr(Path, \"cwd\", staticmethod(_fake_cwd))\n",
    "    df = pd.DataFrame({\"x\": [1, 2, 3]})\n",
    "    nb.save_df_to_csv(df, \"alpha.csv\")\n",
    "    expected = tmp_path / \"proj\" / \"output\" / \"alpha.csv\"\n",
    "    assert expected.exists()\n",
    "    pd.testing.assert_frame_equal(pd.read_csv(expected), df)\n",
    "\n",
    "# ---- download_file (mock network) -------------------------------------------\n",
    "\n",
    "def test_download_file_streams_bytes(tmp_path, monkeypatch, caplog):\n",
    "    caplog.set_level(\"INFO\")\n",
    "    monkeypatch.setattr(nb, \"DEST_ROOT\", str(tmp_path / \"data\"))\n",
    "\n",
    "    # Fake requests.get\n",
    "    class _Resp:\n",
    "        def __init__(self, content: bytes):\n",
    "            self._bio = io.BytesIO(content)\n",
    "            self.status_code = 200\n",
    "        def raise_for_status(self): pass\n",
    "        def iter_content(self, chunk_size=8192):\n",
    "            while True:\n",
    "                chunk = self._bio.read(chunk_size)\n",
    "                if not chunk: break\n",
    "                yield chunk\n",
    "        def __enter__(self): return self\n",
    "        def __exit__(self, *a): pass\n",
    "\n",
    "    def _fake_get(url, stream=True, timeout=60):\n",
    "        return _Resp(b\"hello world\")\n",
    "\n",
    "    monkeypatch.setattr(nb.requests, \"get\", _fake_get)\n",
    "    nb.download_file(\"http://example.com/x.bin\", filename=\"x.bin\")\n",
    "    out = Path(nb.DEST_ROOT) / \"x.bin\"\n",
    "    assert out.exists() and out.read_bytes() == b\"hello world\"\n",
    "\n",
    "# ---- fetch_centroid_latlon (mock ArcGIS + geopandas) -------------------------\n",
    "\n",
    "@pytest.mark.parametrize(\"multi_feature\", [False, True])\n",
    "def test_fetch_centroid_latlon_mock(monkeypatch, multi_feature):\n",
    "    try:\n",
    "        import geopandas as gpd\n",
    "        from shapely.geometry import Polygon\n",
    "    except Exception:\n",
    "        pytest.skip(\"geopandas/shapely not available\")\n",
    "\n",
    "    features = [{\n",
    "        \"type\":\"Feature\",\n",
    "        \"geometry\":{\"type\":\"Polygon\",\"coordinates\":[[(0,0),(1,0),(1,1),(0,1),(0,0)]]},\n",
    "        \"properties\":{}\n",
    "    }]\n",
    "    if multi_feature:\n",
    "        features = features * 2\n",
    "    fake_json = {\"features\": features}\n",
    "\n",
    "    class _Resp:\n",
    "        def raise_for_status(self): pass\n",
    "        def json(self): return fake_json\n",
    "\n",
    "    monkeypatch.setattr(nb.requests, \"get\", lambda *a, **k: _Resp())\n",
    "\n",
    "    # Use a projected CRS to avoid centroid-on-geographic warnings\n",
    "    lat, lon = nb.fetch_centroid_latlon(\n",
    "        suburb_name=\"ANY\",\n",
    "        featureserver_url=\"http://fake\",\n",
    "        epsg_utm=3857,\n",
    "        req_timeout=1,\n",
    "    )\n",
    "    assert pytest.approx(lat, abs=1e-3) == 0.5\n",
    "    assert pytest.approx(lon, abs=1e-3) == 0.5\n",
    "\n",
    "# ---- haversine_km ------------------------------------------------------------\n",
    "\n",
    "def test_haversine_km_basic():\n",
    "    # Distance from same point is ~0\n",
    "    d0 = nb.haversine_km(0, 0, 0, 0)\n",
    "    assert d0 == pytest.approx(0.0, abs=1e-9)\n",
    "\n",
    "    # Rough Cape Town City Hall to Table Mountain Lower Cableway\n",
    "    ct_hall = (-33.9253, 18.4222)\n",
    "    cable = (-33.9703, 18.4031)\n",
    "    d = nb.haversine_km(ct_hall[0], ct_hall[1], cable[0], cable[1])\n",
    "    assert 4.0 < d < 7.5  # sanity band (~5.5 km)\n",
    "\n",
    "# ---- filter_within_radius_km -------------------------------------------------\n",
    "\n",
    "def test_filter_within_radius_km_filters_and_option(tmp_path, caplog):\n",
    "    caplog.set_level(\"INFO\")\n",
    "    cent = (-33.9249, 18.4241)  # Cape Town CBD approx\n",
    "    df = pd.DataFrame({\n",
    "        \"latitude\":  [cent[0], cent[0] + 0.02, np.nan],\n",
    "        \"longitude\": [cent[1], cent[1] + 0.02, 18.5],\n",
    "        \"id\": [1, 2, 3],\n",
    "    })\n",
    "    out = nb.filter_within_radius_km(df, cent[0], cent[1], radius_km=1.852, keep_distance_col=False)\n",
    "    # ~0.02 deg lat ~ 2.22 km => should exclude id=2, keep id=1 only\n",
    "    assert list(out[\"id\"]) == [1]\n",
    "    assert \"distance_km\" not in out.columns\n",
    "\n",
    "# ---- _infer_site_wind_cols ---------------------------------------------------\n",
    "\n",
    "def test_infer_site_wind_cols_picks_shortest_match():\n",
    "    cols = [\n",
    "        \"DateTime\",\n",
    "        \"Bellville_South_AQM_Site_Deg\",\n",
    "        \"Bellville_South_AQM_Site_m/s\",\n",
    "        \"Bellville_South_AQM_Site_Deg_extra\",\n",
    "    ]\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    dcol, scol = nb._infer_site_wind_cols(df, \"Bellville_South\")\n",
    "    assert dcol == \"Bellville_South_AQM_Site_Deg\"\n",
    "    assert scol == \"Bellville_South_AQM_Site_m/s\"\n",
    "\n",
    "# ---- standardize_wind_column_names / fill_wind_missing -----------------------\n",
    "\n",
    "def test_standardize_and_fill_wind_missing():\n",
    "    rng = pd.date_range(\"2020-01-01\", periods=5, freq=\"h\")\n",
    "    df = pd.DataFrame({\n",
    "        \"DateTime\": rng,\n",
    "        \"Bellville_South_AQM_Site_Deg\": [0, np.nan, 90, np.nan, 180],\n",
    "        \"Bellville_South_AQM_Site_m/s\": [1.0, np.nan, 2.0, np.nan, 3.0],\n",
    "    }).set_index(\"DateTime\")\n",
    "\n",
    "    std = nb.standardize_wind_column_names(df, \"Bellville_South\")\n",
    "    assert {\"wind direction degree\", \"wind speed m/s\"} <= set(std.columns)\n",
    "\n",
    "    filled = nb.fill_wind_missing(std.copy())\n",
    "    # After time interpolation, the NaNs should be filled\n",
    "    assert filled[\"wind speed m/s\"].isna().sum() == 0\n",
    "    assert filled[\"wind direction degree\"].isna().sum() == 0\n",
    "\n",
    "# =============================================================================\n",
    "# NEW/UPDATED TESTS FOR REFACTORED HELPERS\n",
    "# =============================================================================\n",
    "\n",
    "def test_prep_sr_timestamps_rounding_and_cutoff():\n",
    "    sr = pd.DataFrame({\n",
    "        \"creation_timestamp\": [\"2020-12-31 22:40\", \"2021-01-01 08:00\", \"bad-ts\"]\n",
    "    })\n",
    "    out = nb.prep_sr_timestamps(sr, ts_col=\"creation_timestamp\",\n",
    "                                out_round_col=\"creation_rounded\",\n",
    "                                cutoff=pd.Timestamp(\"2020-12-31 23:00\"))\n",
    "\n",
    "    # invalid ts dropped\n",
    "    assert len(out) == 2\n",
    "    # first < cutoff -> round to 22:30\n",
    "    assert pd.Timestamp(out.loc[out.index[0], \"creation_rounded\"]) == pd.Timestamp(\"2020-12-31 22:30\")\n",
    "    # second >= cutoff -> forced to 23:00\n",
    "    assert pd.Timestamp(out.loc[out.index[1], \"creation_rounded\"]) == pd.Timestamp(\"2020-12-31 23:00\")\n",
    "\n",
    "def test_build_half_hour_grid_with_circular_dir_behaviour():\n",
    "    # Two consecutive hourly records:\n",
    "    # 22:00: dir=350°, speed=3\n",
    "    # 23:00: dir=10°,  speed=1\n",
    "    # At 22:30 we expect circular average dir ≈ 0°, and speed = arithmetic mean = 2.0\n",
    "    idx = pd.to_datetime([\"2020-12-31 22:00\", \"2020-12-31 23:00\"])\n",
    "    aqm = pd.DataFrame({\n",
    "        \"wind direction degree\": [350.0, 10.0],\n",
    "        \"wind speed m/s\": [3.0, 1.0],\n",
    "        \"other\": [100.0, 200.0],\n",
    "    }, index=idx)\n",
    "\n",
    "    grid = nb.build_half_hour_grid_with_circular_dir(aqm)\n",
    "\n",
    "    # :00 rows preserved\n",
    "    assert pd.Timestamp(\"2020-12-31 22:00\") in grid.index\n",
    "    assert pd.Timestamp(\"2020-12-31 23:00\") in grid.index\n",
    "    # :30 row created\n",
    "    mid = grid.loc[pd.Timestamp(\"2020-12-31 22:30\")]\n",
    "    # circular direction ≈ 0 (or 360)\n",
    "    assert (abs(((mid[\"wind direction degree\"] - 0) + 180) % 360 - 180)) < 1e-6\n",
    "    # speed arithmetic mean\n",
    "    assert mid[\"wind speed m/s\"] == pytest.approx(2.0, 1e-9)\n",
    "    # other numeric columns arithmetic mean\n",
    "    assert mid[\"other\"] == pytest.approx((100 + 200) / 2.0, 1e-9)\n",
    "\n",
    "def test_build_half_hour_grid_with_circular_dir_nan_semantics():\n",
    "    # If one endpoint dir is NaN, :30 dir should be NaN (preserve old mean semantics)\n",
    "    idx = pd.to_datetime([\"2020-01-01 00:00\", \"2020-01-01 01:00\"])\n",
    "    aqm = pd.DataFrame({\n",
    "        \"wind direction degree\": [np.nan, 90.0],\n",
    "        \"wind speed m/s\": [2.0, 4.0],\n",
    "    }, index=idx)\n",
    "\n",
    "    grid = nb.build_half_hour_grid_with_circular_dir(aqm)\n",
    "    mid = grid.loc[pd.Timestamp(\"2020-01-01 00:30\")]\n",
    "\n",
    "    assert np.isnan(mid[\"wind direction degree\"])\n",
    "    # speed arithmetic mean -> NaN if one is NaN? (here both speeds present -> 3.0)\n",
    "    assert mid[\"wind speed m/s\"] == pytest.approx(3.0, 1e-9)\n",
    "\n",
    "# ---- join_sr_to_aqm_fast -----------------------------------------------------\n",
    "\n",
    "def test_join_sr_to_aqm_fast_minimal_updated(caplog):\n",
    "    caplog.set_level(\"INFO\")\n",
    "    # SR: two times, one before cutoff (round to :30), one after (forced to cutoff)\n",
    "    sr = pd.DataFrame({\"creation_timestamp\": [\"2020-12-31 22:40\", \"2021-01-01 08:00\"]})\n",
    "\n",
    "    # AQM hourly: provide speed/dir on the hour\n",
    "    idx = pd.date_range(\"2020-12-31 21:00\", periods=4, freq=\"h\")\n",
    "    aqm = pd.DataFrame({\n",
    "        \"wind direction degree\": [300.0, 350.0, 10.0, 60.0],\n",
    "        \"wind speed m/s\": [1.0, 2.0, 3.0, 4.0],\n",
    "    }, index=idx)\n",
    "\n",
    "    out = nb.join_sr_to_aqm_fast(sr, aqm, cutoff=pd.Timestamp(\"2020-12-31 23:00\"))\n",
    "\n",
    "    # Expect two rows out\n",
    "    assert len(out) == 2\n",
    "    assert \"creation_rounded\" in out.columns\n",
    "\n",
    "    # First SR (22:40) -> round(\"30min\") -> 22:30; speed should be arithmetic mean (2.0 and 3.0 -> 2.5)\n",
    "    first = out.iloc[0]\n",
    "    assert pd.to_datetime(first[\"creation_rounded\"]) == pd.Timestamp(\"2020-12-31 22:30\")\n",
    "    assert first[\"wind speed m/s\"] == pytest.approx((2.0 + 3.0) / 2.0, 1e-9)\n",
    "\n",
    "    # Direction at 22:30 should circular‑average 350° and 10° -> ≈ 0°\n",
    "    assert (abs(((first[\"wind direction degree\"] - 0) + 180) % 360 - 180)) < 1e-6\n",
    "\n",
    "    # Second SR (>= cutoff) -> forced to cutoff 23:00; should match 23:00 row exactly\n",
    "    second = out.iloc[1]\n",
    "    assert pd.to_datetime(second[\"creation_rounded\"]) == pd.Timestamp(\"2020-12-31 23:00\")\n",
    "    assert second[\"wind speed m/s\"] == pytest.approx(3.0, 1e-9)\n",
    "    assert second[\"wind direction degree\"] == pytest.approx(10.0, 1e-9)\n",
    "\n",
    "# ---- anonymize_sr_data -------------------------------------------------------\n",
    "\n",
    "def test_anonymize_sr_data_drops_cols_and_blocks_time():\n",
    "    df = pd.DataFrame({\n",
    "        \"notification_number\":[1],\n",
    "        \"reference_number\":[2],\n",
    "        \"distance_km\":[0.5],\n",
    "        \"creation_rounded\":[\"2020-01-01 00:00\"],\n",
    "        \"latitude\":[-33.9],\n",
    "        \"longitude\":[18.4],\n",
    "        \"code\":[\"X\"],\n",
    "        \"cause_code\":[\"Y\"],\n",
    "        \"h3_level8_index\":[\"88ad...\"],\n",
    "        \"creation_timestamp\":[\"2020-01-01 01:23:45+02:00\"],\n",
    "        \"completion_timestamp\":[\"2020-01-01 06:01:00+02:00\"],\n",
    "        \"other\":[123],\n",
    "    })\n",
    "    out = nb.anonymize_sr_data(df)\n",
    "\n",
    "    # Dropped columns\n",
    "    for c in [\"notification_number\",\"reference_number\",\"distance_km\",\n",
    "              \"creation_rounded\",\"latitude\",\"longitude\",\"code\",\"cause_code\"]:\n",
    "        assert c not in out.columns\n",
    "\n",
    "    # Timestamps floored to 6h blocks (timezone-naive after processing)\n",
    "    c0 = pd.to_datetime(out.loc[0, \"creation_timestamp\"])\n",
    "    c1 = pd.to_datetime(out.loc[0, \"completion_timestamp\"])\n",
    "    assert c0 == pd.Timestamp(\"2020-01-01 00:00\")\n",
    "    assert c1 == pd.Timestamp(\"2020-01-01 06:00\")\n",
    "\n",
    "    # Preserved other fields\n",
    "    assert out.loc[0, \"h3_level8_index\"] == \"88ad...\"\n",
    "    assert out.loc[0, \"other\"] == 123\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
