{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# --- Stdlib ---\n",
    "import contextlib\n",
    "import gc\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Hashable, Iterable, Literal, Mapping, Optional, Sequence, Tuple\n",
    "\n",
    "# --- Main ---\n",
    "import __main__ as nb\n",
    "\n",
    "# --- Data Science ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "\n",
    "# --- Testing ---\n",
    "import ipytest\n",
    "import pytest\n",
    "\n",
    "# --- Geo ---\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "\n",
    "# --- Optional Resources ---\n",
    "try:\n",
    "    import psutil as _psutil\n",
    "except Exception:\n",
    "    _psutil = None\n",
    "\n",
    "try:\n",
    "    import tracemalloc as _tracemalloc\n",
    "except Exception:\n",
    "    _tracemalloc = None\n",
    "\n",
    "# --- Config ---\n",
    "ipytest.autoconfig()\n",
    "\n",
    "# --- Logging setup ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Utilities ---\n",
    "def ensure_dir(path: str | os.PathLike) -> None:\n",
    "    os.makedirs(path if isinstance(path, str) else str(path), exist_ok=True)\n",
    "\n",
    "def parent_dir_of(path: str | os.PathLike) -> str:\n",
    "    return os.path.dirname(str(path)) or \".\"\n",
    "\n",
    "def fmt_bytes(n: int | float) -> str:\n",
    "    try:\n",
    "        n = float(n)\n",
    "    except Exception:\n",
    "        return str(n)\n",
    "    for unit in (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"):\n",
    "        if abs(n) < 1024 or unit == \"PB\":\n",
    "            return f\"{n:.1f}{unit}\"\n",
    "        n /= 1024\n",
    "\n",
    "def gunzip_file(src: str, dst: str) -> None:\n",
    "    ensure_dir(parent_dir_of(dst))\n",
    "    with gzip.open(src, \"rb\") as fin, open(dst, \"wb\") as fout:\n",
    "        shutil.copyfileobj(fin, fout)\n",
    "\n",
    "def client_error_msg(e) -> str:\n",
    "    code = getattr(e, \"response\", {}).get(\"Error\", {}).get(\"Code\")\n",
    "    msg  = getattr(e, \"response\", {}).get(\"Error\", {}).get(\"Message\")\n",
    "    return f\"[{code}] {msg}\" if code or msg else str(e)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timed(label: str, enabled: bool = True):\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if enabled:\n",
    "            logger.info(\"%s took %.3f s\", label, time.perf_counter() - t0)\n",
    "\n",
    "def resource_snapshot(note: str = \"\") -> None:\n",
    "    if not logger.isEnabledFor(logging.DEBUG):\n",
    "        return\n",
    "    parts: list[str] = []\n",
    "    if _psutil:\n",
    "        try:\n",
    "            p = _psutil.Process(os.getpid())\n",
    "            parts.append(f\"rss={fmt_bytes(p.memory_info().rss)}\")\n",
    "            parts.append(f\"cpu%~{p.cpu_percent(interval=0.0):.1f}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if _tracemalloc and _tracemalloc.is_tracing():\n",
    "        try:\n",
    "            cur, peak = _tracemalloc.get_traced_memory()\n",
    "            parts.append(f\"py_mem={fmt_bytes(cur)}/{fmt_bytes(peak)}(peak)\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if parts:\n",
    "        logger.debug(\"RES%s %s\", f\"[{note}]\" if note else \"\", \" \".join(parts))\n",
    "\n",
    "# --- Project defaults ---\n",
    "DEST_ROOT = \"../data\"\n",
    "\n",
    "def resolve_path(rel: str | None, filename: str) -> str:\n",
    "    if rel:\n",
    "        return str(Path(rel) / filename)\n",
    "    return str(Path(DEST_ROOT) / filename)\n",
    "\n",
    "def load_project_files(\n",
    "    file_map: dict[str, str],\n",
    "    project_root_name: str = \"ds_code_challenge\",\n",
    "    inject_globals: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load files from ROOT/data based on a {file_name: variable_name} map.\n",
    "\n",
    "    Supports:\n",
    "      - .csv / .csv.gz  -> pandas DataFrame\n",
    "      - .geojson / .geojson.gz -> GeoDataFrame\n",
    "      - .ods -> Excel (odf engine)\n",
    "    \"\"\"\n",
    "    with timed(\"load_project_files (resolve ROOT/DATA_DIR)\"):\n",
    "        ROOT = Path(__file__).resolve().parents[0] if \"__file__\" in globals() else Path().resolve()\n",
    "        while ROOT.name != project_root_name and ROOT.parent != ROOT:\n",
    "            ROOT = ROOT.parent\n",
    "        DATA_DIR = ROOT / \"data\"\n",
    "\n",
    "    results: dict[str, pd.DataFrame | gpd.GeoDataFrame] = {}\n",
    "    resource_snapshot(\"load_project_files:start\")\n",
    "\n",
    "    for file_name, var_name in file_map.items():\n",
    "        file_path = DATA_DIR / file_name\n",
    "        logger.info(\"Processing %s...\", file_path)\n",
    "\n",
    "        with timed(f\"read:{file_path.name}\"):\n",
    "            suffix = \"\".join(file_path.suffixes).lower()\n",
    "\n",
    "            if suffix.endswith((\".csv\", \".csv.gz\")):\n",
    "                df = pd.read_csv(file_path)\n",
    "                results[var_name] = df\n",
    "                logger.info(\"→ %s loaded as DataFrame shape=%s\", var_name, getattr(df, \"shape\", None))\n",
    "\n",
    "            elif suffix.endswith((\".geojson\", \".geojson.gz\")):\n",
    "                gdf = gpd.read_file(file_path)\n",
    "                results[var_name] = gdf\n",
    "                logger.info(\"→ %s loaded as GeoDataFrame len=%d\", var_name, len(gdf))\n",
    "\n",
    "            elif suffix.endswith(\".ods\"):\n",
    "                df = pd.read_excel(file_path, engine=\"odf\")\n",
    "                results[var_name] = df\n",
    "                logger.info(\"→ %s loaded from ODS shape=%s\", var_name, getattr(df, \"shape\", None))\n",
    "\n",
    "            else:\n",
    "                logger.warning(\"Skipping unsupported file type: %s\", file_path)\n",
    "                continue\n",
    "\n",
    "            if inject_globals:\n",
    "                globals()[var_name] = results[var_name]\n",
    "\n",
    "            resource_snapshot(f\"after_load:{file_path.name}\")\n",
    "\n",
    "    logger.info(\"All files loaded successfully.\")\n",
    "    resource_snapshot(\"load_project_files:end\")\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_map = {\n",
    "    \"sr_hex.csv\": \"df_sr_hex\",\n",
    "    \"sr.csv\": \"df_sr\"\n",
    "}\n",
    "datasets = load_project_files(file_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "def assign_h3_level8_sr(\n",
    "    df: pd.DataFrame,\n",
    "    lat_col: str = \"latitude\",\n",
    "    lon_col: str = \"longitude\",\n",
    "    *,\n",
    "    resolution: int = 8,\n",
    "    out_col: str | None = None,\n",
    "    invalid_token: str = \"0\",\n",
    "    validate_bounds: bool = True,\n",
    "    threshold: float = 0.05,\n",
    "    early_abort: bool = True,\n",
    "    copy_frame: bool = True,\n",
    "    log_validation: bool = True,\n",
    "    log_join: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attach an H3 (v4) index at the given `resolution` to each (lat, lon) row.\n",
    "\n",
    "    Integrations with your support functions:\n",
    "      • Wraps the whole op in `timed()` and emits `resource_snapshot()` at start/end (DEBUG only).\n",
    "      • Uses your global `logger` formatting, with toggles for validation/join logs.\n",
    "      • Avoids extra passes by deriving join stats from the validity mask.\n",
    "      • Allows in-place mutation (copy_frame=False) for speed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input frame containing latitude/longitude columns.\n",
    "    lat_col, lon_col : str\n",
    "        Column names for coordinates.\n",
    "    resolution : int, default 8\n",
    "        H3 resolution. Name is kept generic; function still called assign_h3_level8.\n",
    "    out_col : str | None\n",
    "        Destination column name. Defaults to f\"h3_level{resolution}_index\".\n",
    "    invalid_token : str, default \"0\"\n",
    "        Value to assign when (lat, lon) are invalid/missing/out-of-bounds.\n",
    "    validate_bounds : bool, default True\n",
    "        If True, require -90<=lat<=90 and -180<=lon<=180 in addition to finite checks.\n",
    "    threshold : float, default 0.05\n",
    "        Max tolerated share of invalid rows before failing.\n",
    "    early_abort : bool, default True\n",
    "        If True, raise immediately when invalid share exceeds `threshold`.\n",
    "    copy_frame : bool, default True\n",
    "        If True, operate on a copy; otherwise mutate `df` in place.\n",
    "    log_validation, log_join : bool\n",
    "        Toggle INFO-level summary logs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Frame with the new `out_col` (string H3 index).\n",
    "    \"\"\"\n",
    "\n",
    "    # Column existence\n",
    "    if lat_col not in df.columns or lon_col not in df.columns:\n",
    "        raise ValueError(f\"Missing required columns: {lat_col!r}, {lon_col!r}\")\n",
    "\n",
    "    # Output column default derived from resolution\n",
    "    out_col = out_col or f\"h3_level{resolution}_index\"\n",
    "\n",
    "    with timed(f\"assign_h3_level{resolution}\"):\n",
    "        if logger.isEnabledFor(logging.DEBUG):\n",
    "            resource_snapshot(\"start\")\n",
    "\n",
    "        # Choose frame to write into\n",
    "        df_out = df.copy() if copy_frame else df\n",
    "\n",
    "        # --- Validation (single pass to arrays) ---\n",
    "        lat = pd.to_numeric(df_out[lat_col], errors=\"coerce\").to_numpy()\n",
    "        lon = pd.to_numeric(df_out[lon_col], errors=\"coerce\").to_numpy()\n",
    "\n",
    "        finite = np.isfinite(lat) & np.isfinite(lon)\n",
    "        if validate_bounds:\n",
    "            in_bounds = (lat >= -90.0) & (lat <= 90.0) & (lon >= -180.0) & (lon <= 180.0)\n",
    "            valid = finite & in_bounds\n",
    "        else:\n",
    "            valid = finite\n",
    "\n",
    "        n = int(lat.shape[0])\n",
    "        n_valid = int(valid.sum())\n",
    "        n_invalid = n - n_valid\n",
    "        fail_rate_pre = (n_invalid / n) if n else 0.0\n",
    "\n",
    "        if log_validation:\n",
    "            logger.info(\n",
    "                \"Validation: rows=%d valid=%d failed_join=%d (%.2f%% invalid)\",\n",
    "                n, n_valid, n_invalid, fail_rate_pre * 100,\n",
    "            )\n",
    "\n",
    "        if early_abort and fail_rate_pre > threshold:\n",
    "            if logger.isEnabledFor(logging.DEBUG):\n",
    "                resource_snapshot(\"abort\")\n",
    "            raise RuntimeError(\n",
    "                f\"Failure rate {fail_rate_pre:.2%} exceeds threshold {threshold:.2%}\"\n",
    "            )\n",
    "\n",
    "        # --- Compute H3 (fill invalids with token; compute only on valid mask) ---\n",
    "        # Keep object dtype for safety with string indexes; can switch to pandas StringDtype if desired.\n",
    "        out = np.empty(n, dtype=object)\n",
    "        if n_invalid:\n",
    "            out[~valid] = invalid_token\n",
    "\n",
    "        if n_valid:\n",
    "            # List comp over masked arrays is typically fastest for h3 python bindings\n",
    "            out[valid] = [\n",
    "                h3.latlng_to_cell(float(la), float(lo), int(resolution))\n",
    "                for la, lo in zip(lat[valid], lon[valid])\n",
    "            ]\n",
    "\n",
    "        # Single assignment (no extra scan)\n",
    "        df_out[out_col] = out\n",
    "\n",
    "        # Join stats (same as validation stats here, since invalid => token)\n",
    "        success = n_valid\n",
    "        failed = n_invalid\n",
    "        failed_rate = (failed / n) if n else 0.0\n",
    "\n",
    "        if log_join:\n",
    "            logger.info(\n",
    "                \"Join stats: success=%d failed=%d (%.2f%%)\",\n",
    "                success, failed, failed_rate * 100,\n",
    "            )\n",
    "\n",
    "        if n and failed_rate > threshold:\n",
    "            # This is a late gate in case you *didn't* early abort\n",
    "            raise RuntimeError(\n",
    "                f\"Too many failed joins: {failed_rate:.2%} exceeds threshold {threshold:.2%}\"\n",
    "            )\n",
    "\n",
    "        if logger.isEnabledFor(logging.DEBUG):\n",
    "            resource_snapshot(\"end\")\n",
    "\n",
    "        return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0924c5",
   "metadata": {},
   "source": [
    "Using direct H3 conversion was much faster than doing a direct join. Also doesn't give any mismatches anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2962bb",
   "metadata": {},
   "source": [
    "This function compares two df's based on keys and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Sequence, Mapping, Hashable, Literal, Optional\n",
    "\n",
    "def compare_dfs_by_keys(\n",
    "    df1: pd.DataFrame,\n",
    "    df2: pd.DataFrame,\n",
    "    keys: str | Sequence[str],\n",
    "    cols: Optional[Sequence[str]] = None,\n",
    "    *,\n",
    "    # Equality controls\n",
    "    na_equal: bool = True,\n",
    "    coerce_to_str: bool = False,\n",
    "    case_insensitive: bool = False,\n",
    "    numeric_atol: float | None = None,            # absolute tol\n",
    "    numeric_rtol: float | None = None,            # relative tol (np.isclose-style)\n",
    "    per_column_atol: Optional[Mapping[str, float]] = None,\n",
    "    datetime_tol: pd.Timedelta | None = None,     # e.g., pd.Timedelta(\"1ms\")\n",
    "    # Join & prep\n",
    "    drop_duplicate_keys: bool = False,\n",
    "    how: Literal[\"outer\", \"inner\", \"left\", \"right\"] = \"outer\",\n",
    "    # Logging\n",
    "    log_summary: bool = True,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compare two DataFrames by `keys` across one or many `cols`.\n",
    "\n",
    "    Improvements:\n",
    "      • Uses your `timed()` + `resource_snapshot()` for instrumentation (DEBUG only).\n",
    "      • Tolerant comparisons:\n",
    "          – Numeric: np.isclose with per-column overrides (`per_column_atol`) and global atol/rtol.\n",
    "          – Datetime: |Δ| <= `datetime_tol` if provided.\n",
    "      • Safe string handling with optional case-insensitive compare.\n",
    "      • Duplicate-key control to avoid cartesian blowups.\n",
    "      • Clear, compact summary with match rates per column.\n",
    "    \"\"\"\n",
    "    with timed(\"compare_dfs_by_keys\"):\n",
    "        if logger.isEnabledFor(logging.DEBUG):\n",
    "            resource_snapshot(\"start\")\n",
    "\n",
    "        # --- Normalize & guards ---\n",
    "        if isinstance(keys, str):\n",
    "            keys = [keys]\n",
    "        keys = list(keys)\n",
    "\n",
    "        for k in keys:\n",
    "            if k not in df1.columns or k not in df2.columns:\n",
    "                raise ValueError(f\"Key '{k}' must exist in both DataFrames.\")\n",
    "\n",
    "        if cols is None:\n",
    "            shared = set(df1.columns).intersection(df2.columns) - set(keys)\n",
    "            if not shared:\n",
    "                raise ValueError(\"No shared columns to compare (excluding keys). Provide 'cols'.\")\n",
    "            cols = sorted(shared)\n",
    "        else:\n",
    "            for c in cols:\n",
    "                if c not in df1.columns or c not in df2.columns:\n",
    "                    raise ValueError(f\"Column '{c}' must exist in both DataFrames.\")\n",
    "            cols = list(cols)\n",
    "\n",
    "        # --- Slice & (optional) dedupe on keys ---\n",
    "        left = df1[keys + cols].copy()\n",
    "        right = df2[keys + cols].copy()\n",
    "        if drop_duplicate_keys:\n",
    "            left = left.drop_duplicates(subset=keys, keep=\"first\")\n",
    "            right = right.drop_duplicates(subset=keys, keep=\"first\")\n",
    "\n",
    "        # --- Optional coercions ---\n",
    "        def _to_str_preserve_na(s: pd.Series) -> pd.Series:\n",
    "            # keeps NA as NA, stringifies everything else\n",
    "            return s.astype(\"object\").where(s.isna(), s.astype(str))\n",
    "\n",
    "        if coerce_to_str:\n",
    "            for c in cols:\n",
    "                left[c] = _to_str_preserve_na(left[c])\n",
    "                right[c] = _to_str_preserve_na(right[c])\n",
    "\n",
    "        if case_insensitive:\n",
    "            for c in cols:\n",
    "                if (pd.api.types.is_object_dtype(left[c]) or pd.api.types.is_string_dtype(left[c])):\n",
    "                    left[c]  = left[c].where(left[c].isna(),  left[c].astype(str).str.casefold())\n",
    "                    right[c] = right[c].where(right[c].isna(), right[c].astype(str).str.casefold())\n",
    "\n",
    "        # --- Merge (configurable 'how') ---\n",
    "        merged = left.merge(right, on=keys, how=how, suffixes=(\"_df1\", \"_df2\"), indicator=True)\n",
    "\n",
    "        only_in_df1 = merged.loc[merged[\"_merge\"] == \"left_only\",  keys].reset_index(drop=True)\n",
    "        only_in_df2 = merged.loc[merged[\"_merge\"] == \"right_only\", keys].reset_index(drop=True)\n",
    "        both_mask = merged[\"_merge\"] == \"both\"\n",
    "\n",
    "        both_cols = keys + [f\"{c}_df1\" for c in cols] + [f\"{c}_df2\" for c in cols]\n",
    "        both = merged.loc[both_mask, both_cols].copy()\n",
    "\n",
    "        # --- Column-wise equality (tolerant for numeric/datetime) ---\n",
    "        equal_masks: dict[str, pd.Series] = {}\n",
    "        for c in cols:\n",
    "            l = both[f\"{c}_df1\"]\n",
    "            r = both[f\"{c}_df2\"]\n",
    "\n",
    "            # Handle datetimes with tolerance if requested\n",
    "            is_dt_l = pd.api.types.is_datetime64_any_dtype(l)\n",
    "            is_dt_r = pd.api.types.is_datetime64_any_dtype(r)\n",
    "            if is_dt_l and is_dt_r and datetime_tol is not None:\n",
    "                # NAs equal?\n",
    "                if na_equal:\n",
    "                    na_eq = l.isna() & r.isna()\n",
    "                else:\n",
    "                    na_eq = pd.Series(False, index=l.index)\n",
    "                non_na = ~(l.isna() | r.isna())\n",
    "                eq_dt = pd.Series(False, index=l.index)\n",
    "                if non_na.any():\n",
    "                    eq_dt.loc[non_na] = (l[non_na] - r[non_na]).abs() <= datetime_tol\n",
    "                equal_masks[c] = na_eq | eq_dt\n",
    "                continue\n",
    "\n",
    "            # Numeric with np.isclose\n",
    "            is_num_l = pd.api.types.is_numeric_dtype(l)\n",
    "            is_num_r = pd.api.types.is_numeric_dtype(r)\n",
    "            if is_num_l and is_num_r and (numeric_atol is not None or numeric_rtol is not None or (per_column_atol and c in per_column_atol)):\n",
    "                if na_equal:\n",
    "                    na_eq = l.isna() & r.isna()\n",
    "                else:\n",
    "                    na_eq = pd.Series(False, index=l.index)\n",
    "                non_na = ~(l.isna() | r.isna())\n",
    "                eq_num = pd.Series(False, index=l.index)\n",
    "                if non_na.any():\n",
    "                    atol = per_column_atol.get(c) if per_column_atol else None\n",
    "                    atol = numeric_atol if atol is None else atol\n",
    "                    rtol = numeric_rtol if numeric_rtol is not None else 0.0\n",
    "                    eq_num.loc[non_na] = np.isclose(l[non_na].astype(float), r[non_na].astype(float),\n",
    "                                                    atol=(0.0 if atol is None else float(atol)),\n",
    "                                                    rtol=float(rtol),\n",
    "                                                    equal_nan=False)\n",
    "                equal_masks[c] = na_eq | eq_num\n",
    "                continue\n",
    "\n",
    "            # Fallback: exact equality (with NA-equality if requested)\n",
    "            eq = (l == r)\n",
    "            if na_equal:\n",
    "                eq = eq | (l.isna() & r.isna())\n",
    "            equal_masks[c] = eq\n",
    "\n",
    "        # --- Keys where ALL compared columns match ---\n",
    "        if cols:\n",
    "            all_equal_mask = np.logical_and.reduce([equal_masks[c].to_numpy() for c in cols])\n",
    "        else:\n",
    "            all_equal_mask = np.array([], dtype=bool)\n",
    "\n",
    "        matches_all = both.loc[all_equal_mask, keys].reset_index(drop=True)\n",
    "\n",
    "        # --- Long-form mismatches ---\n",
    "        rows = []\n",
    "        for c in cols:\n",
    "            neq = ~equal_masks[c]\n",
    "            if not neq.any():\n",
    "                continue\n",
    "            sub = both.loc[neq, keys + [f\"{c}_df1\", f\"{c}_df2\"]].copy()\n",
    "            sub.insert(len(keys), \"column\", c)\n",
    "            sub = sub.rename(columns={f\"{c}_df1\": \"left\", f\"{c}_df2\": \"right\"})\n",
    "\n",
    "            # Add abs_diff for numeric, |Δ| for datetimes if types align\n",
    "            if pd.api.types.is_numeric_dtype(sub[\"left\"]) and pd.api.types.is_numeric_dtype(sub[\"right\"]):\n",
    "                sub[\"abs_diff\"] = (sub[\"left\"] - sub[\"right\"]).abs()\n",
    "            elif pd.api.types.is_datetime64_any_dtype(sub[\"left\"]) and pd.api.types.is_datetime64_any_dtype(sub[\"right\"]):\n",
    "                sub[\"abs_diff\"] = (sub[\"left\"] - sub[\"right\"]).abs()\n",
    "            else:\n",
    "                sub[\"abs_diff\"] = pd.NA\n",
    "\n",
    "            rows.append(sub)\n",
    "\n",
    "        mismatches_long = (\n",
    "            pd.concat(rows, ignore_index=True)\n",
    "            if rows\n",
    "            else pd.DataFrame(columns=[*keys, \"column\", \"left\", \"right\", \"abs_diff\"])\n",
    "        )\n",
    "\n",
    "        # --- Summary per column ---\n",
    "        total_both = len(both)\n",
    "        summary = pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"column\": c,\n",
    "                    \"rows_in_both\": total_both,\n",
    "                    \"matches\": int(equal_masks[c].sum()),\n",
    "                    \"mismatches\": int((~equal_masks[c]).sum()),\n",
    "                    \"match_rate\": (float(equal_masks[c].mean()) if total_both else np.nan),\n",
    "                }\n",
    "                for c in cols\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if log_summary:\n",
    "            logger.info(\n",
    "                \"Compare summary: rows_in_both=%d | avg_match_rate=%.2f%%\",\n",
    "                total_both,\n",
    "                100.0 * (summary[\"match_rate\"].mean() if not summary.empty else np.nan),\n",
    "            )\n",
    "\n",
    "        if logger.isEnabledFor(logging.DEBUG):\n",
    "            resource_snapshot(\"end\")\n",
    "\n",
    "        return {\n",
    "            \"only_in_df1\": only_in_df1,\n",
    "            \"only_in_df2\": only_in_df2,\n",
    "            \"both_rows\": both.reset_index(drop=True),\n",
    "            \"matches_all\": matches_all,\n",
    "            \"mismatches_long\": mismatches_long.reset_index(drop=True),\n",
    "            \"summary\": summary.reset_index(drop=True),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sr_h3_l8 = assign_h3_level8_sr(df_sr, lat_col=\"latitude\", lon_col=\"longitude\", threshold=0.226, copy_frame=False, log_join=False)\n",
    "df_sr_h3_l8.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa36842",
   "metadata": {},
   "source": [
    "Threshold was chosen at 22.6% as 22.55% failed to join. Thus I chose a higher threshold so that the code would not error out early. The default if we don't set a threshold is 5% as this would give us a confidence level of 95%. \n",
    "Assignment of the h3 levels just used h3.latlng_to_cell instead of doing joining to the geojson file as it was much faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5341da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "comparison_df = compare_dfs_by_keys(df_sr_h3_l8, df_sr_hex, keys=\"notification_number\", cols=[\"h3_level8_index\"])\n",
    "\n",
    "print(comparison_df[\"summary\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b82ce6",
   "metadata": {},
   "source": [
    "Unit tests of functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ipytest -q -rA\n",
    "\n",
    "\n",
    "# --- Make sure the notebook-defined function sees required globals ---\n",
    "def _wire_globals_for_notebook_func():\n",
    "    fn = nb.assign_h3_level8_sr\n",
    "\n",
    "    # Provide pandas if not in function globals\n",
    "    if \"pd\" not in fn.__globals__:\n",
    "        fn.__globals__[\"pd\"] = pd\n",
    "\n",
    "    # Lightweight logger stub\n",
    "    class _Logger:\n",
    "        def debug(self, *a, **k): pass\n",
    "        def info(self, *a, **k): pass\n",
    "        def error(self, *a, **k): pass\n",
    "\n",
    "    if \"logger\" not in fn.__globals__:\n",
    "        fn.__globals__[\"logger\"] = _Logger()\n",
    "\n",
    "    \n",
    "    if \"error\" not in fn.__globals__:\n",
    "        fn.__globals__[\"error\"] = lambda *a, **k: None\n",
    "\n",
    "_wire_globals_for_notebook_func()\n",
    "\n",
    "# Use real h3 if available; otherwise skip these tests cleanly\n",
    "h3 = pytest.importorskip(\"h3\")\n",
    "\n",
    "def test_valid_coords_return_h3_cell():\n",
    "    df = pd.DataFrame({\"latitude\": [-33.9249], \"longitude\": [18.4241]})\n",
    "    out = nb.assign_h3_level8_sr(df, resolution=8, threshold=0.0, early_abort=False)\n",
    "    assert out.loc[0, \"h3_level8_index\"] == h3.latlng_to_cell(-33.9249, 18.4241, 8)\n",
    "    \n",
    "def test_known_point_exact_index_level8():\n",
    "    lat, lon = -33.872839, 18.522488\n",
    "    expected = \"88ad360225fffff\"\n",
    "    df = pd.DataFrame({\"latitude\": [lat], \"longitude\": [lon]})\n",
    "    out = nb.assign_h3_level8_sr(df, resolution=8, threshold=0.0, early_abort=False)\n",
    "    assert out.loc[0, \"h3_level8_index\"] == expected\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"lat,lon\", [\n",
    "    (np.nan, 18.4),      \n",
    "    (-33.9, np.nan),     \n",
    "    (None, 18.4),        \n",
    "    (-33.9, None),       \n",
    "])\n",
    "def test_nan_lat_or_lon_produces_zero(lat, lon):\n",
    "    df = pd.DataFrame({\"latitude\": [lat], \"longitude\": [lon]})\n",
    "    out = nb.assign_h3_level8_sr(df, threshold=1.0, early_abort=False)  \n",
    "    assert out.loc[0, \"h3_level8_index\"] == \"0\"\n",
    "\n",
    "def test_out_of_range_coords_produce_zero():\n",
    "    df = pd.DataFrame({\n",
    "        \"latitude\":  [-91.0,  0.0,   45.0],\n",
    "        \"longitude\": [  0.0, 200.0,  np.nan],\n",
    "    })\n",
    "    out = nb.assign_h3_level8_sr(df, threshold=1.0, early_abort=False)\n",
    "    assert out[\"h3_level8_index\"].tolist() == [\"0\", \"0\", \"0\"]\n",
    "\n",
    "def test_precheck_early_abort_on_high_invalid_ratio():\n",
    "    \n",
    "    df = pd.DataFrame({\"latitude\": [100.0, -33.9], \"longitude\": [0.0, 18.4]})\n",
    "    with pytest.raises(RuntimeError, match=\"exceeds threshold\"):\n",
    "        nb.assign_h3_level8_sr(df, threshold=0.4, early_abort=True)\n",
    "\n",
    "def test_resolution_changes_output():\n",
    "    df = pd.DataFrame({\"latitude\": [-33.9249], \"longitude\": [18.4241]})\n",
    "\n",
    "    out7 = nb.assign_h3_level8_sr(df, resolution=7, threshold=0.0, early_abort=False)\n",
    "    out8 = nb.assign_h3_level8_sr(df, resolution=8, threshold=0.0, early_abort=False)\n",
    "\n",
    "    col7 = \"h3_level7_index\"\n",
    "    col8 = \"h3_level8_index\"\n",
    "\n",
    "    assert col7 in out7.columns\n",
    "    assert col8 in out8.columns\n",
    "    assert out7.loc[0, col7] != out8.loc[0, col8]\n",
    "\n",
    "def test_missing_columns_raise_value_error():\n",
    "    df = pd.DataFrame({\"latitude\": [0.0]})  \n",
    "    with pytest.raises(ValueError):\n",
    "        nb.assign_h3_level8_sr(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
